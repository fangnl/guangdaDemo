# 合并后文件的大小 不设置默认合并为一个文件
# mapreduce.input.fileinputformat.split.maxsize=0
# 集群的位置
# 使用spark合并的阈值
spark.merge.size=31457280
#fs.defaultFS.value=hdfs://mycluster:8020
#存放shell的路径
shell.path=/root/shell/exceMerge.sh
# jar报的位置
shell.spark.jar.location=/root/SparkMerge-1.0-SNAPSHOT-jar-with-dependencies.jar
# 主程序的名称
shell.spark.mainClass=com.guangda.SparkMerge
#命令
shell.spark.submit.command=./spark-submit --master yarn --deploy-mode cluster --class 
# 需要配置多个合并路径用逗号隔开
hdfs.file.intPutPath=/textoneG,/megerlittlefile2/test01
# 对应顺序的输出路径用逗号隔开输出路径不能相同
hdfs.file.outPutPath=/megerlittlefile2/test16,/megerlittlefile2/test17
# spark on yarn 的配置
# driver的内存
spark.driver-memory=1g
# executor 的内存
spark.executor-memory=1g
#executor 的核数
spark.executor-cores=1
# executor的数量
spark.num-executors=2
# 设置spark的并行度

linux.host=node4
linux.userName=root
linux.password=123456
linux.myPort=22





