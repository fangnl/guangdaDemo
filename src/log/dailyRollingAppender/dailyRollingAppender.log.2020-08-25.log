[2020-08-25 19:10:27:597] [INFO ] [method:com.guangda.mergeFile.DemoTest.main(DemoTest.java:24)]hello word
[2020-08-25 19:10:27:598] [INFO ] [method:com.guangda.mergeFile.DemoTest.main(DemoTest.java:25)]hello word
[2020-08-25 19:10:27:598] [ERROR] [method:com.guangda.mergeFile.DemoTest.main(DemoTest.java:26)]hello word
[2020-08-25 19:20:36:774] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.guangda.shellUtil.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.guangda.shellUtil.MergeFilePath.<init>(MergeFilePath.java:20)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:107)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 19:20:36:790] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 19:20:36:790] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 19:20:36:835] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 19:20:36:840] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 19:20:36:841] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 19:20:36:844] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 19:20:36:845] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 19:20:36:846] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 19:20:36:918] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 19:20:36:924] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 19:20:36:928] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 19:20:36:928] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 19:20:36:929] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 19:20:36:930] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 19:20:36:931] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 19:20:36:932] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 19:20:37:009] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 19:20:37:019] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 19:20:37:020] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 19:20:37:021] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 19:20:37:021] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 19:20:37:021] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 19:20:37:022] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 19:20:37:073] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-25 19:20:37:079] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-25 19:20:37:334] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:20:37:334] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:20:37:334] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:20:37:334] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:20:37:334] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:20:37:334] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:20:37:335] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:20:37:335] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:20:37:335] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:20:37:335] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:20:37:336] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:20:37:336] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:20:37:343] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-25 19:20:37:343] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-25 19:20:37:343] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-25 19:20:37:358] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-25 19:20:37:359] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-25 19:20:37:359] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-25 19:20:37:359] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:20:37:359] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:20:37:360] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:20:37:360] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:20:37:360] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:20:37:360] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:20:37:360] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:20:37:360] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:20:37:361] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:20:37:361] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:20:37:361] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:20:37:361] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:20:37:372] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 19:20:37:372] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 19:20:37:372] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 19:20:37:386] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@1927d74d
[2020-08-25 19:20:37:395] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@2f5bc552
[2020-08-25 19:20:37:396] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@2f5bc552
[2020-08-25 19:20:37:396] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@2f5bc552
[2020-08-25 19:20:37:726] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-25 19:20:37:730] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-25 19:20:37:730] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-25 19:20:37:730] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-25 19:20:37:736] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@2f5bc552
[2020-08-25 19:20:37:736] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@2f5bc552
[2020-08-25 19:20:37:755] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 19:20:37:755] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 19:20:37:755] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 19:20:37:756] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-25 19:20:37:799] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-25 19:20:37:801] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-25 19:20:37:804] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:20:37:805] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-25 19:20:37:809] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #1
[2020-08-25 19:20:37:809] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #2
[2020-08-25 19:20:37:809] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 69ms
[2020-08-25 19:20:37:810] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #0
[2020-08-25 19:20:37:810] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 70ms
[2020-08-25 19:20:37:810] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 70ms
[2020-08-25 19:20:37:835] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:20:37:836] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:20:37:837] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #4
[2020-08-25 19:20:37:837] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-25 19:20:37:839] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #3
[2020-08-25 19:20:37:839] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 4ms
[2020-08-25 19:20:37:840] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:20:37:841] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #5
[2020-08-25 19:20:37:841] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-25 19:20:37:842] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:20:37:843] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #6
[2020-08-25 19:20:37:843] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-25 19:20:37:845] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1273)]/megerlittlefile2/test04/part-m-00000: masked=rw-r--r--
[2020-08-25 19:20:37:896] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
[2020-08-25 19:20:37:936] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #7
[2020-08-25 19:20:37:936] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: create took 40ms
[2020-08-25 19:20:37:957] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.computePacketChunkSize(DFSOutputStream.java:389)]computePacketChunkSize: src=/megerlittlefile2/test04/part-m-00000, chunkSize=516, chunksPerPacket=126, packetSize=65016
[2020-08-25 19:20:37:963] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)]Lease renewer daemon for [DFSClient_NONMAPREDUCE_528997844_12] with renew id 1 started
[2020-08-25 19:20:37:979] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 19:20:37:990] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #8
[2020-08-25 19:20:37:991] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 13ms
[2020-08-25 19:20:38:014] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 19:20:38:017] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 19:20:38:036] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
[2020-08-25 19:20:38:037] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #9
[2020-08-25 19:20:38:038] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getServerDefaults took 2ms
[2020-08-25 19:20:38:048] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-25 19:20:38:134] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:418)]DFSClient writeChunk allocating new packet seqno=0, src=/megerlittlefile2/test04/part-m-00000, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
[2020-08-25 19:20:38:136] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 19:20:38:138] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #10
[2020-08-25 19:20:38:138] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 2ms
[2020-08-25 19:20:38:139] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 19:20:38:139] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.72:50010
[2020-08-25 19:20:38:140] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.72, datanodeId = DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]
[2020-08-25 19:20:38:215] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 19:20:38:219] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #11
[2020-08-25 19:20:38:219] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 4ms
[2020-08-25 19:20:38:221] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 19:20:38:222] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.74:50010
[2020-08-25 19:20:38:226] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.74, datanodeId = DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-25 19:20:38:295] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 0
[2020-08-25 19:20:38:295] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:703)]Allocating new block
[2020-08-25 19:20:38:295] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 1
[2020-08-25 19:20:38:296] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:867)]Waiting for ack for: 1
[2020-08-25 19:20:38:306] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #12 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock
[2020-08-25 19:20:38:315] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #12
[2020-08-25 19:20:38:315] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: addBlock took 9ms
[2020-08-25 19:20:38:317] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1679)]pipeline = [DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]
[2020-08-25 19:20:38:317] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:255)]Connecting to datanode 192.168.163.72:50010
[2020-08-25 19:20:38:320] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:267)]Send buf size 131768
[2020-08-25 19:20:38:320] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.72, datanodeId = DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]
[2020-08-25 19:20:38:396] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761568_20862 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 26445
[2020-08-25 19:20:38:517] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 14629170
[2020-08-25 19:20:38:518] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761568_20862 sending packet packet seqno: 1 offsetInBlock: 26445 lastPacketInBlock: true lastByteOffsetInBlock: 26445
[2020-08-25 19:20:38:539] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 4017535
[2020-08-25 19:20:38:539] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:622)]Closing old block BP-972878040-192.168.163.71-1586355896590:blk_1073761568_20862
[2020-08-25 19:20:38:541] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root sending #13 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete
[2020-08-25 19:20:38:548] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root got value #13
[2020-08-25 19:20:38:548] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: complete took 7ms
[2020-08-25 19:20:48:547] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root: closed
[2020-08-25 19:20:48:548] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2126348932) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-25 19:21:08:070] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-25 19:21:38:164] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-25 19:21:39:170] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:473)]Lease renewer daemon for [] with renew id 1 expired
[2020-08-25 19:21:39:170] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:318)]Lease renewer daemon for [] with renew id 1 exited
[2020-08-25 19:23:25:684] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@2f5bc552
[2020-08-25 19:23:25:686] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@2f5bc552
[2020-08-25 19:23:25:686] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@2f5bc552
[2020-08-25 19:23:25:687] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-25 19:23:25:688] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-25 19:27:08:929] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.guangda.shellUtil.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.guangda.shellUtil.MergeFilePath.<init>(MergeFilePath.java:20)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:107)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 19:27:08:941] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 19:27:08:941] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 19:27:08:983] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 19:27:08:990] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 19:27:08:991] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 19:27:08:991] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 19:27:08:992] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 19:27:08:994] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 19:27:09:058] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 19:27:09:062] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 19:27:09:065] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 19:27:09:067] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 19:27:09:067] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 19:27:09:071] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 19:27:09:075] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 19:27:09:076] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 19:27:09:155] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 19:27:09:164] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 19:27:09:164] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 19:27:09:166] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 19:27:09:166] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 19:27:09:166] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 19:27:09:167] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 19:27:09:230] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-25 19:27:09:237] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-25 19:27:09:482] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:27:09:482] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:27:09:482] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:27:09:482] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:27:09:482] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:27:09:482] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:27:09:482] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:27:09:482] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:27:09:482] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:27:09:483] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:27:09:483] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:27:09:483] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:27:09:489] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-25 19:27:09:489] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-25 19:27:09:489] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-25 19:27:09:504] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-25 19:27:09:504] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-25 19:27:09:504] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-25 19:27:09:504] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:27:09:504] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:27:09:504] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:27:09:504] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 19:27:09:505] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:27:09:505] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:27:09:505] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:27:09:505] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 19:27:09:505] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:27:09:506] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:27:09:505] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 19:27:09:506] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 19:27:09:515] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 19:27:09:515] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 19:27:09:515] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 19:27:09:531] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@324cd508
[2020-08-25 19:27:09:541] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@27b912d
[2020-08-25 19:27:09:541] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@27b912d
[2020-08-25 19:27:09:542] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@27b912d
[2020-08-25 19:27:09:971] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-25 19:27:09:981] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-25 19:27:09:981] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-25 19:27:09:981] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-25 19:27:09:992] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@27b912d
[2020-08-25 19:27:09:992] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@27b912d
[2020-08-25 19:27:10:031] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 19:27:10:031] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 19:27:10:031] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 19:27:10:033] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-25 19:27:10:079] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (42547299) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-25 19:27:10:080] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-25 19:27:10:084] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-25 19:27:10:085] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:27:10:089] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #1
[2020-08-25 19:27:10:090] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 90ms
[2020-08-25 19:27:10:090] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #0
[2020-08-25 19:27:10:090] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 90ms
[2020-08-25 19:27:10:090] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #2
[2020-08-25 19:27:10:091] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 91ms
[2020-08-25 19:27:10:112] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:27:10:113] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:27:10:113] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:27:10:114] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #4
[2020-08-25 19:27:10:115] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-25 19:27:10:115] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #5
[2020-08-25 19:27:10:115] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #3
[2020-08-25 19:27:10:115] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-25 19:27:10:116] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 4ms
[2020-08-25 19:27:10:116] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:27:10:119] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #6
[2020-08-25 19:27:10:119] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-25 19:27:10:119] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 19:27:10:122] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #7
[2020-08-25 19:27:10:123] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 4ms
[2020-08-25 19:27:10:125] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1273)]/megerlittlefile2/test07/part-m-00000: masked=rw-r--r--
[2020-08-25 19:27:10:166] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
[2020-08-25 19:27:10:180] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #8
[2020-08-25 19:27:10:180] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: create took 40ms
[2020-08-25 19:27:10:203] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.computePacketChunkSize(DFSOutputStream.java:389)]computePacketChunkSize: src=/megerlittlefile2/test07/part-m-00000, chunkSize=516, chunksPerPacket=126, packetSize=65016
[2020-08-25 19:27:10:211] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)]Lease renewer daemon for [DFSClient_NONMAPREDUCE_-604436246_11] with renew id 1 started
[2020-08-25 19:27:10:226] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 19:27:10:229] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #9
[2020-08-25 19:27:10:229] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 3ms
[2020-08-25 19:27:10:248] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 19:27:10:251] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.74:50010
[2020-08-25 19:27:10:269] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
[2020-08-25 19:27:10:270] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #10
[2020-08-25 19:27:10:271] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getServerDefaults took 3ms
[2020-08-25 19:27:10:281] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.74, datanodeId = DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-25 19:27:10:385] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:418)]DFSClient writeChunk allocating new packet seqno=0, src=/megerlittlefile2/test07/part-m-00000, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
[2020-08-25 19:27:10:389] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 19:27:10:392] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #11
[2020-08-25 19:27:10:393] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 4ms
[2020-08-25 19:27:10:394] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 19:27:10:395] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.72:50010
[2020-08-25 19:27:10:397] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.72, datanodeId = DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]
[2020-08-25 19:27:10:416] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #12 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 19:27:10:417] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #12
[2020-08-25 19:27:10:417] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 1ms
[2020-08-25 19:27:10:418] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 19:27:10:418] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 19:27:10:419] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-25 19:27:10:427] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 0
[2020-08-25 19:27:10:427] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:703)]Allocating new block
[2020-08-25 19:27:10:427] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 1
[2020-08-25 19:27:10:427] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:867)]Waiting for ack for: 1
[2020-08-25 19:27:10:435] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #13 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock
[2020-08-25 19:27:10:438] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #13
[2020-08-25 19:27:10:439] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: addBlock took 5ms
[2020-08-25 19:27:10:440] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1679)]pipeline = [DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]
[2020-08-25 19:27:10:441] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:255)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 19:27:10:442] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:267)]Send buf size 131768
[2020-08-25 19:27:10:442] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-25 19:27:10:459] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761580_20874 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 26445
[2020-08-25 19:27:10:518] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 5661721
[2020-08-25 19:27:10:519] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761580_20874 sending packet packet seqno: 1 offsetInBlock: 26445 lastPacketInBlock: true lastByteOffsetInBlock: 26445
[2020-08-25 19:27:10:526] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 4513716
[2020-08-25 19:27:10:526] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:622)]Closing old block BP-972878040-192.168.163.71-1586355896590:blk_1073761580_20874
[2020-08-25 19:27:10:528] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (42547299) connection to /192.168.163.71:8020 from root sending #14 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete
[2020-08-25 19:27:10:531] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (42547299) connection to /192.168.163.71:8020 from root got value #14
[2020-08-25 19:27:10:531] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: complete took 4ms
[2020-08-25 19:27:20:532] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (42547299) connection to /192.168.163.71:8020 from root: closed
[2020-08-25 19:27:20:534] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (42547299) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-25 19:27:40:302] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-25 19:28:10:399] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-25 19:28:11:405] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:473)]Lease renewer daemon for [] with renew id 1 expired
[2020-08-25 19:28:11:405] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:318)]Lease renewer daemon for [] with renew id 1 exited
[2020-08-25 19:32:15:448] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@27b912d
[2020-08-25 19:32:15:459] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@27b912d
[2020-08-25 19:32:15:461] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@27b912d
[2020-08-25 19:32:15:461] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-25 19:32:15:468] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-25 20:33:07:054] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:33:07:060] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:33:07:131] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:33:07:217] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 20:33:07:227] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 20:33:07:227] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 20:33:07:227] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 20:33:07:228] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 20:33:07:229] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:33:07:417] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:26)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:14)
[2020-08-25 20:33:07:500] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:33:07:500] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:33:07:561] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:33:07:568] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:33:07:572] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:33:07:573] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:33:07:574] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:33:07:575] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:33:07:576] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:33:07:577] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:33:07:682] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:33:07:696] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:33:07:697] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:33:07:700] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:33:07:701] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:33:07:701] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:33:07:702] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:33:07:723] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:33:07:843] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:33:07:844] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:33:07:845] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:33:07:845] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:33:07:846] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:33:07:863] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:33:07:954] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:33:07:960] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:33:07:960] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:33:08:012] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:33:08:044] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:33:08:046] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:33:08:047] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:33:08:048] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:33:08:049] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:33:08:049] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:33:08:050] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:33:08:050] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:33:08:051] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:33:08:051] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:33:08:051] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:33:08:051] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:33:08:051] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:33:08:052] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:33:08:053] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:33:08:053] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:33:08:054] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:33:08:069] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:33:08:069] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:33:08:078] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:33:08:105] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:33:08:105] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:33:08:107] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:33:08:107] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:33:08:108] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:33:08:108] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:33:08:108] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:33:08:108] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:33:08:108] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:33:08:108] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:33:08:108] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:33:08:108] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:33:08:108] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:33:08:153] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75647 (auto-detected)
[2020-08-25 20:33:08:155] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:33:08:156] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:33:08:159] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:33:08:160] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:33:08:165] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:33:08:204] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:33:08:205] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:33:08:205] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:33:08:226] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:230] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:232] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:236] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:238] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:239] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:242] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:244] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:246] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:248] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:252] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:255] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:257] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:260] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:262] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:265] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:33:08:274] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:33:08:289] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:38:51:798] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:38:51:803] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:38:51:853] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:38:51:922] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 20:38:51:929] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 20:38:51:930] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 20:38:51:930] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 20:38:51:930] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 20:38:51:931] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:38:52:010] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:31)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:17)
[2020-08-25 20:38:52:069] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:38:52:069] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:38:52:096] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:38:52:099] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:38:52:100] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:38:52:101] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:38:52:102] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:38:52:103] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:38:52:103] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:38:52:104] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:38:52:168] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:38:52:173] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:38:52:174] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:38:52:174] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:38:52:175] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:38:52:175] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:38:52:176] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:38:52:185] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:38:52:257] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:38:52:258] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:38:52:258] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:38:52:259] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:38:52:260] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:38:52:272] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:38:52:354] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:38:52:356] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:38:52:357] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:38:52:385] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:38:52:413] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:38:52:415] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:38:52:416] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:38:52:417] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:38:52:418] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:38:52:419] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:38:52:419] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:38:52:420] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:38:52:420] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:38:52:420] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:38:52:421] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:38:52:421] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:38:52:421] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:38:52:422] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:38:52:423] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:38:52:423] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:38:52:424] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:38:52:440] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:38:52:440] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:38:52:450] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:38:52:485] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:38:52:485] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:38:52:488] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:38:52:488] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:38:52:489] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:38:52:489] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:38:52:489] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:38:52:490] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:38:52:490] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:38:52:490] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:38:52:490] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:38:52:490] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:38:52:490] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:38:52:561] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75703 (auto-detected)
[2020-08-25 20:38:52:564] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:38:52:564] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:38:52:568] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:38:52:570] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:38:52:574] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:38:52:612] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:38:52:613] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:38:52:613] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:38:52:626] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:629] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:631] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:632] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:633] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:635] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:636] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:637] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:639] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:640] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:641] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:642] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:643] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:644] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:645] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:647] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:38:52:651] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:38:52:655] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:39:33:677] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:39:33:681] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:39:33:728] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:39:33:797] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 20:39:33:807] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 20:39:33:807] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 20:39:33:808] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 20:39:33:808] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 20:39:33:809] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:39:33:920] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:31)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:18)
[2020-08-25 20:39:33:998] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:39:33:999] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:39:34:051] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:39:34:054] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:39:34:056] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:39:34:057] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:39:34:058] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:39:34:059] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:39:34:059] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:39:34:060] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:39:34:136] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:39:34:142] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:39:34:143] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:39:34:144] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:39:34:144] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:39:34:144] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:39:34:145] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:39:34:158] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:39:34:260] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:39:34:260] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:39:34:261] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:39:34:262] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:39:34:263] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:39:34:275] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:39:34:367] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:39:34:370] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:39:34:371] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:39:34:418] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:39:34:465] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:39:34:468] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:39:34:470] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:39:34:476] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:39:34:478] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:39:34:479] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:39:34:481] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:39:34:483] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:39:34:483] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:39:34:483] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:39:34:483] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:39:34:484] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:39:34:485] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:39:34:486] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:39:34:487] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:39:34:487] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:39:34:488] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:39:34:525] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:39:34:526] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:39:34:544] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:39:34:597] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:39:34:598] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:39:34:603] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:39:34:603] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:39:34:603] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:39:34:603] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:39:34:604] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:39:34:604] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:39:34:604] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:39:34:604] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:39:34:605] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:39:34:605] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:39:34:605] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:39:34:695] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75709 (auto-detected)
[2020-08-25 20:39:34:698] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:39:34:698] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:39:34:702] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:39:34:703] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:39:34:707] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:39:34:741] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:39:34:741] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:39:34:741] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:39:34:758] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:762] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:765] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:767] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:768] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:770] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:771] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:772] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:774] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:775] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:776] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:777] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:779] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:780] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:782] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:784] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:34:788] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:39:34:792] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:39:49:889] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:39:49:893] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:39:49:938] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:39:49:996] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-25 20:39:50:004] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-25 20:39:50:005] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-25 20:39:50:005] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-25 20:39:50:006] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-25 20:39:50:007] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:39:50:097] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:31)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:18)
[2020-08-25 20:39:50:207] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:39:50:208] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:39:50:247] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:39:50:253] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:39:50:257] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:39:50:259] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:39:50:261] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:39:50:262] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:39:50:264] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:39:50:266] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:39:50:332] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:39:50:337] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:39:50:337] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:39:50:338] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:39:50:339] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:39:50:339] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:39:50:339] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:39:50:347] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:39:50:401] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:39:50:402] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:39:50:403] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:39:50:404] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:39:50:404] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:39:50:416] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:39:50:497] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:39:50:502] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:39:50:502] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:39:50:533] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:39:50:561] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:39:50:563] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:39:50:564] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:39:50:565] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:39:50:566] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:39:50:566] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:39:50:567] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:39:50:568] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:39:50:568] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:39:50:568] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:39:50:568] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:39:50:569] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:39:50:569] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:39:50:570] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:39:50:571] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:39:50:571] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:39:50:572] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:39:50:587] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:39:50:588] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:39:50:597] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:39:50:646] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:39:50:647] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:39:50:650] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:39:50:651] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:39:50:652] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:39:50:652] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:39:50:652] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:39:50:652] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:39:50:652] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:39:50:653] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:39:50:653] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:39:50:653] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:39:50:653] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:39:50:744] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75714 (auto-detected)
[2020-08-25 20:39:50:748] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:39:50:749] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:39:50:753] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:39:50:754] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:39:50:759] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:39:50:785] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:39:50:785] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:39:50:785] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:39:50:799] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:803] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:806] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:810] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:813] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:818] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:820] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:822] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:823] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:824] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:826] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:827] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:828] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:830] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:831] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:833] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:39:50:837] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:39:50:841] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:41:13:584] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:41:13:591] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:41:13:653] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:41:13:731] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-25 20:41:13:740] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-25 20:41:13:741] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-25 20:41:13:741] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-25 20:41:13:742] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-25 20:41:13:743] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:41:13:846] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:31)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:18)
[2020-08-25 20:41:13:906] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:41:13:906] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:41:13:940] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:41:13:946] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:41:13:948] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:41:13:949] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:41:13:952] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:41:13:952] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:41:13:953] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:41:13:954] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:41:14:025] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:41:14:030] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:41:14:031] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:41:14:032] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:41:14:033] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:41:14:033] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:41:14:034] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:41:14:043] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:41:14:171] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:41:14:173] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:41:14:175] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:41:14:178] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:41:14:180] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:41:14:217] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:41:14:278] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:41:14:280] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:41:14:281] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:41:14:307] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:41:14:333] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:41:14:334] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:41:14:335] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:41:14:337] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:41:14:338] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:41:14:339] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:41:14:339] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:41:14:340] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:41:14:341] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:41:14:341] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:41:14:341] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:41:14:341] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:41:14:342] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:41:14:343] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:41:14:343] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:41:14:343] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:41:14:344] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:41:14:386] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:41:14:387] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:41:14:402] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:41:14:453] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:41:14:453] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:41:14:457] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:41:14:457] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:41:14:457] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:41:14:458] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:41:14:458] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:41:14:458] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:41:14:458] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:41:14:458] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:41:14:458] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:41:14:459] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:41:14:459] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:41:14:537] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75738 (auto-detected)
[2020-08-25 20:41:14:539] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:41:14:540] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:41:14:543] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:41:14:544] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:41:14:548] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:41:14:575] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:41:14:575] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:41:14:576] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:41:14:622] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:624] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:628] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:631] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:633] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:637] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:640] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:644] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:646] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:649] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:651] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:653] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:656] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:658] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:663] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:665] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:41:14:672] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute$$$capture(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:41:14:682] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:43:54:158] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:43:54:163] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:43:54:215] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:43:54:284] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 20:43:54:292] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 20:43:54:293] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 20:43:54:293] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 20:43:54:294] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 20:43:54:295] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:43:54:380] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:31)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:18)
[2020-08-25 20:43:54:440] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:43:54:440] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:43:54:477] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:43:54:480] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:43:54:483] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:43:54:484] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:43:54:486] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:43:54:486] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:43:54:487] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:43:54:487] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:43:54:568] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:43:54:574] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:43:54:575] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:43:54:576] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:43:54:576] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:43:54:577] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:43:54:577] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:43:54:588] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:43:54:676] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:43:54:678] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:43:54:680] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:43:54:681] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:43:54:682] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:43:54:695] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:43:54:765] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:43:54:767] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:43:54:767] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:43:54:794] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:43:54:835] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:43:54:837] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:43:54:837] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:43:54:840] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:43:54:842] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:43:54:843] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:43:54:843] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:43:54:844] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:43:54:844] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:43:54:844] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:43:54:845] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:43:54:845] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:43:54:846] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:43:54:848] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:43:54:849] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:43:54:849] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:43:54:850] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:43:54:882] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:43:54:883] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:43:54:902] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:43:54:962] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:43:54:962] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:43:54:965] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:43:54:965] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:43:54:966] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:43:54:966] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:43:54:966] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:43:54:966] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:43:54:966] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:43:54:967] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:43:54:967] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:43:54:967] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:43:54:967] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:43:55:036] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75757 (auto-detected)
[2020-08-25 20:43:55:048] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:43:55:049] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:43:55:054] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:43:55:057] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:43:55:065] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:43:55:119] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:43:55:119] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:43:55:120] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:43:55:136] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:140] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:142] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:144] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:146] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:147] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:149] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:151] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:152] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:153] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:155] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:156] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:157] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:159] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:160] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:162] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:43:55:166] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:43:55:171] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:48:56:107] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:48:56:114] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:48:56:164] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:48:56:231] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 20:48:56:239] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 20:48:56:239] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 20:48:56:240] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 20:48:56:240] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 20:48:56:241] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:48:56:349] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:31)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:18)
[2020-08-25 20:48:56:427] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:48:56:427] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:48:56:461] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:48:56:464] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:48:56:466] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:48:56:467] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:48:56:469] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:48:56:470] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:48:56:470] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:48:56:471] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:48:56:545] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:48:56:550] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:48:56:550] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:48:56:551] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:48:56:551] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:48:56:551] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:48:56:552] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:48:56:561] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:48:56:635] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:48:56:636] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:48:56:636] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:48:56:637] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:48:56:637] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:48:56:650] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:48:56:736] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:48:56:741] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:48:56:741] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:48:56:769] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:48:56:813] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:48:56:815] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:48:56:816] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:48:56:818] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:48:56:819] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:48:56:820] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:48:56:821] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:48:56:822] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:48:56:823] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:48:56:823] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:48:56:823] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:48:56:823] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:48:56:824] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:48:56:825] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:48:56:825] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:48:56:826] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:48:56:828] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:48:56:854] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:48:56:854] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:48:56:870] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:48:56:914] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:48:56:914] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:48:56:917] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:48:56:917] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:48:56:917] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:48:56:917] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:48:56:918] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:48:56:918] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:48:56:918] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:48:56:918] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:48:56:918] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:48:56:918] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:48:56:919] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:48:56:969] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75797 (auto-detected)
[2020-08-25 20:48:56:974] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:48:56:974] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:48:56:977] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:48:56:978] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:48:56:983] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:48:57:025] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:48:57:025] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:48:57:025] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:48:57:051] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:056] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:059] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:062] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:065] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:068] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:071] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:074] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:077] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:080] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:084] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:087] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:091] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:094] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:098] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:103] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:48:57:115] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:48:57:123] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:54:43:829] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:54:43:833] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:54:43:879] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:54:43:939] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 20:54:43:945] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 20:54:43:946] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 20:54:43:946] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 20:54:43:947] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 20:54:43:947] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:54:44:040] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:32)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:18)
[2020-08-25 20:54:44:139] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:54:44:140] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:54:44:187] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:54:44:191] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:54:44:193] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:54:44:193] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:54:44:195] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:54:44:196] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:54:44:196] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:54:44:197] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:54:44:314] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:54:44:328] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:54:44:329] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:54:44:332] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:54:44:333] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:54:44:334] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:54:44:336] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:54:44:363] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:54:44:463] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:54:44:464] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:54:44:465] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:54:44:465] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:54:44:466] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:54:44:479] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:54:44:592] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:54:44:595] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:54:44:596] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:54:44:643] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:54:44:681] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:54:44:683] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:54:44:684] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:54:44:685] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:54:44:686] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:54:44:687] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:54:44:687] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:54:44:688] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:54:44:688] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:54:44:689] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:54:44:689] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:54:44:689] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:54:44:690] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:54:44:691] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:54:44:691] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:54:44:692] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:54:44:693] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:54:44:714] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:54:44:715] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:54:44:726] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:54:44:766] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:54:44:767] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:54:44:770] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:54:44:770] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:54:44:770] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:54:44:771] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:54:44:771] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:54:44:771] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:54:44:771] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:54:44:771] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:54:44:771] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:54:44:771] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:54:44:772] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:54:44:844] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75852 (auto-detected)
[2020-08-25 20:54:44:851] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:54:44:852] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:54:44:859] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:54:44:862] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:54:44:869] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:54:44:933] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:54:44:933] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:54:44:933] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:54:44:948] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:952] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:953] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:955] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:957] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:958] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:960] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:962] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:963] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:964] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:966] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:967] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:968] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:969] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:970] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:972] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:54:44:975] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:54:44:979] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:55:18:548] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:55:18:552] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:55:18:597] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:55:18:660] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-25 20:55:18:668] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-25 20:55:18:668] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-25 20:55:18:669] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-25 20:55:18:669] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-25 20:55:18:670] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:55:18:746] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:33)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:19)
[2020-08-25 20:55:18:818] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:55:18:818] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:55:18:865] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:55:18:869] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:55:18:871] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:55:18:873] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:55:18:875] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:55:18:876] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:55:18:877] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:55:18:878] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:55:19:019] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:55:19:025] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:55:19:027] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:55:19:028] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:55:19:028] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:55:19:028] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:55:19:029] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:55:19:051] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:55:19:174] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:55:19:175] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:55:19:176] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:55:19:176] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:55:19:177] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:55:19:197] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:55:19:271] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:55:19:274] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:55:19:276] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:55:19:311] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:55:19:362] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:55:19:368] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:55:19:370] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:55:19:372] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:55:19:373] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:55:19:374] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:55:19:375] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:55:19:375] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:55:19:376] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:55:19:376] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:55:19:376] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:55:19:377] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:55:19:377] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:55:19:380] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:55:19:382] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:55:19:382] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:55:19:383] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:55:19:412] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:55:19:412] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:55:19:422] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:55:19:451] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:55:19:451] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:55:19:453] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:55:19:454] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:55:19:454] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:55:19:454] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:55:19:454] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:55:19:454] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:55:19:455] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:55:19:455] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:55:19:455] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:55:19:455] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:55:19:455] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:55:19:532] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75860 (auto-detected)
[2020-08-25 20:55:19:540] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:55:19:541] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:55:19:549] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:55:19:551] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:55:19:572] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:55:19:624] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:55:19:625] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:55:19:625] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:55:19:644] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:648] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:650] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:652] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:653] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:655] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:656] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:658] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:660] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:661] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:663] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:664] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:666] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:667] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:668] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:670] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:19:675] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:55:19:680] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:55:51:710] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:55:51:715] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:55:51:763] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:55:51:816] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 20:55:51:823] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 20:55:51:823] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 20:55:51:824] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 20:55:51:824] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 20:55:51:825] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:55:51:891] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:33)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:19)
[2020-08-25 20:55:51:937] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:55:51:937] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:55:51:971] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:55:51:976] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:55:51:978] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:55:51:979] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:55:51:981] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:55:51:981] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:55:51:982] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:55:51:983] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:55:52:063] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:55:52:068] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:55:52:068] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:55:52:069] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:55:52:069] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:55:52:069] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:55:52:070] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:55:52:077] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:55:52:130] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:55:52:131] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:55:52:132] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:55:52:132] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:55:52:133] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:55:52:143] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:55:52:208] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:55:52:210] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:55:52:211] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:55:52:236] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:55:52:270] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:55:52:274] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:55:52:275] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:55:52:276] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:55:52:277] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:55:52:279] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:55:52:279] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:55:52:280] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:55:52:280] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:55:52:281] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:55:52:281] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:55:52:281] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:55:52:282] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:55:52:283] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:55:52:283] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:55:52:283] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:55:52:284] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:55:52:299] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:55:52:299] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:55:52:307] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:55:52:332] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:55:52:333] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:55:52:335] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:55:52:335] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:55:52:335] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:55:52:335] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:55:52:335] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:55:52:335] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:55:52:336] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:55:52:336] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:55:52:336] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:55:52:336] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:55:52:336] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:55:52:392] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75864 (auto-detected)
[2020-08-25 20:55:52:396] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:55:52:396] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:55:52:401] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:55:52:402] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:55:52:408] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:55:52:447] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:55:52:448] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:55:52:448] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:55:52:466] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:471] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:474] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:476] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:479] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:480] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:482] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:483] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:484] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:485] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:487] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:488] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:489] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:491] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:492] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:493] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:55:52:497] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:55:52:501] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:56:11:619] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:56:11:631] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:56:11:838] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:56:12:156] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 20:56:12:177] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 20:56:12:179] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 20:56:12:181] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 20:56:12:183] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 20:56:12:188] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:56:12:723] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:33)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:19)
[2020-08-25 20:56:12:882] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:56:12:883] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:56:13:279] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:56:13:290] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:56:13:300] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:56:13:303] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:56:13:313] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:56:13:314] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:56:13:317] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:56:13:321] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:56:14:084] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:56:14:091] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:56:14:095] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:56:14:097] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:56:14:098] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:56:14:100] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:56:14:102] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:56:14:168] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:56:14:573] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:56:14:577] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:56:14:581] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:56:14:584] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:56:14:588] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:56:14:657] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:56:14:955] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:56:14:963] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:56:14:964] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:56:15:098] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:56:15:347] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:56:15:353] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:56:15:358] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:56:15:371] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:56:15:376] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:56:15:381] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:56:15:385] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:56:15:391] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:56:15:392] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:56:15:393] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:56:15:395] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:56:15:396] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:56:15:397] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:56:15:408] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:56:15:410] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:56:15:411] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:56:15:416] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:56:15:702] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:56:15:703] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:56:15:839] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:56:16:021] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:56:16:022] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:56:16:032] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:56:16:033] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:56:16:034] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:56:16:035] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:56:16:036] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:56:16:037] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:56:16:038] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:56:16:039] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:56:16:039] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:56:16:040] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:56:16:041] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:56:16:246] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75870 (auto-detected)
[2020-08-25 20:56:16:258] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:56:16:260] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:56:16:274] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:56:16:282] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:56:16:295] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:56:16:492] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:56:16:493] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:56:16:494] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:56:16:579] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:586] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:592] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:598] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:603] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:609] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:621] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:627] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:632] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:637] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:641] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:646] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:650] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:654] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:659] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:665] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:56:16:679] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute$$$capture(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:56:16:715] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 20:56:27:704] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-25 20:56:27:716] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-25 20:57:34:743] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-25 20:57:35:066] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 20:57:35:089] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 20:57:35:091] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 20:57:35:093] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 20:57:35:095] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 20:57:35:099] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 20:57:35:622] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:33)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:19)
[2020-08-25 20:57:35:769] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 20:57:35:770] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 20:57:36:192] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 20:57:36:204] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 20:57:36:213] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 20:57:36:215] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 20:57:36:223] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 20:57:36:224] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 20:57:36:226] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 20:57:36:229] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 20:57:36:923] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 20:57:36:930] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 20:57:36:932] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 20:57:36:934] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 20:57:36:935] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 20:57:36:935] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 20:57:36:937] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 20:57:36:988] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-25 20:57:37:359] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-25 20:57:37:362] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-25 20:57:37:365] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-25 20:57:37:368] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-25 20:57:37:371] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-25 20:57:37:436] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-25 20:57:37:706] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-25 20:57:37:714] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-25 20:57:37:715] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-25 20:57:37:827] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-25 20:57:38:038] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-25 20:57:38:043] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-25 20:57:38:047] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-25 20:57:38:056] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-25 20:57:38:061] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-25 20:57:38:065] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-25 20:57:38:068] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-25 20:57:38:072] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-25 20:57:38:073] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-25 20:57:38:074] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-25 20:57:38:075] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-25 20:57:38:076] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-25 20:57:38:077] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-25 20:57:38:084] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-25 20:57:38:085] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-25 20:57:38:086] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-25 20:57:38:090] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-25 20:57:38:280] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-25 20:57:38:281] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-25 20:57:38:397] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-25 20:57:38:532] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-25 20:57:38:533] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-25 20:57:38:543] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-25 20:57:38:544] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-25 20:57:38:545] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-25 20:57:38:546] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-25 20:57:38:547] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-25 20:57:38:548] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-25 20:57:38:548] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-25 20:57:38:549] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-25 20:57:38:550] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-25 20:57:38:551] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-25 20:57:38:552] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-25 20:57:38:741] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 75873 (auto-detected)
[2020-08-25 20:57:38:751] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-25 20:57:38:752] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-25 20:57:38:764] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-25 20:57:38:770] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-25 20:57:38:782] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-25 20:57:38:983] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-25 20:57:38:985] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-25 20:57:38:986] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-25 20:57:39:064] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:070] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:076] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:081] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:086] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:092] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:108] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:114] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:120] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:125] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:130] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:135] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:139] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:144] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:149] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:156] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-25 20:57:39:173] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute$$$capture(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 20:57:39:214] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-25 21:46:02:646] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.guangda.shellUtil.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.guangda.shellUtil.MergeFilePath.<init>(MergeFilePath.java:20)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:107)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 21:46:02:662] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 21:46:02:663] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 21:46:02:718] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 21:46:02:730] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 21:46:02:731] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 21:46:02:732] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 21:46:02:733] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 21:46:02:738] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 21:46:02:827] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 21:46:02:834] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 21:46:02:837] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 21:46:02:838] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 21:46:02:838] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 21:46:02:840] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 21:46:02:841] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 21:46:02:841] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 21:46:02:917] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 21:46:02:929] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 21:46:02:930] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 21:46:02:931] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 21:46:02:931] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 21:46:02:931] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 21:46:02:933] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 21:46:03:011] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-25 21:46:03:016] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-25 21:46:03:243] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 21:46:03:243] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 21:46:03:243] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 21:46:03:243] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 21:46:03:255] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-25 21:46:03:273] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-25 21:46:03:274] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 21:46:03:274] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 21:46:03:274] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 21:46:03:274] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 21:46:03:287] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 21:46:03:305] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@21461aff
[2020-08-25 21:46:03:317] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@3ed96f41
[2020-08-25 21:46:03:625] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-25 21:46:03:629] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-25 21:46:03:663] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 21:46:03:664] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-25 21:46:03:816] [WARN ] [method:org.apache.hadoop.ipc.Client$Connection.handleConnectionFailure(Client.java:920)]Failed to connect to server: 192.168.163.71/192.168.163.71:8020: try once and fail.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:597)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1698)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1682)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:897)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:112)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:961)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.Globber.listStatus(Globber.java:76)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:234)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.guangda.shellUtil.MergeFilePath.getPatternFile(MergeFilePath.java:25)
	at com.guangda.shellUtil.MergeFilePath.merge(MergeFilePath.java:67)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:111)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 21:46:03:818] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1244)]closing ipc connection to 192.168.163.71/192.168.163.71:8020: Connection refused
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:597)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1698)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1682)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:897)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:112)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:961)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.Globber.listStatus(Globber.java:76)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:234)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.guangda.shellUtil.MergeFilePath.getPatternFile(MergeFilePath.java:25)
	at com.guangda.shellUtil.MergeFilePath.merge(MergeFilePath.java:67)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:111)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 21:46:03:819] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (397639453) connection to /192.168.163.71:8020 from root: closed
[2020-08-25 21:46:03:824] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getListing over /192.168.163.71:8020. Trying to failover immediately.
java.net.ConnectException: Call From localhost/127.0.0.1 to 192.168.163.71:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:597)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1698)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1682)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:897)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:112)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:961)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.Globber.listStatus(Globber.java:76)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:234)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.guangda.shellUtil.MergeFilePath.getPatternFile(MergeFilePath.java:25)
	at com.guangda.shellUtil.MergeFilePath.merge(MergeFilePath.java:67)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:111)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 33 more
[2020-08-25 21:46:03:825] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 21:46:03:825] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@3ed96f41
[2020-08-25 21:46:03:826] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 21:46:03:826] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-25 21:46:03:843] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (397639453) connection to /192.168.163.72:8020 from root: starting, having connections 1
[2020-08-25 21:46:03:846] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 21:46:04:071] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #0
[2020-08-25 21:46:04:071] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 245ms
[2020-08-25 21:46:04:099] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 21:46:04:101] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #1
[2020-08-25 21:46:04:102] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-25 21:46:04:103] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 21:46:04:104] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #2
[2020-08-25 21:46:04:104] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-25 21:46:04:105] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 21:46:04:107] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #3
[2020-08-25 21:46:04:107] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-25 21:46:04:109] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1273)]/megerlittlefile2/test08/part-m-00000: masked=rw-r--r--
[2020-08-25 21:46:04:123] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
[2020-08-25 21:46:04:199] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #4
[2020-08-25 21:46:04:199] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: create took 77ms
[2020-08-25 21:46:04:209] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.computePacketChunkSize(DFSOutputStream.java:389)]computePacketChunkSize: src=/megerlittlefile2/test08/part-m-00000, chunkSize=516, chunksPerPacket=126, packetSize=65016
[2020-08-25 21:46:04:212] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)]Lease renewer daemon for [DFSClient_NONMAPREDUCE_875067922_11] with renew id 1 started
[2020-08-25 21:46:04:217] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 21:46:04:276] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #5
[2020-08-25 21:46:04:276] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 59ms
[2020-08-25 21:46:04:314] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 21:46:04:315] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 21:46:04:324] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
[2020-08-25 21:46:04:371] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #6
[2020-08-25 21:46:04:371] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getServerDefaults took 47ms
[2020-08-25 21:46:04:376] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-25 21:46:04:449] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:418)]DFSClient writeChunk allocating new packet seqno=0, src=/megerlittlefile2/test08/part-m-00000, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
[2020-08-25 21:46:04:451] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 21:46:04:454] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #7
[2020-08-25 21:46:04:454] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 3ms
[2020-08-25 21:46:04:455] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 21:46:04:455] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 21:46:04:458] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 21:46:04:459] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #8
[2020-08-25 21:46:04:460] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 2ms
[2020-08-25 21:46:04:460] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 21:46:04:460] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 21:46:04:464] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 0
[2020-08-25 21:46:04:464] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:703)]Allocating new block
[2020-08-25 21:46:04:464] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 1
[2020-08-25 21:46:04:465] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:867)]Waiting for ack for: 1
[2020-08-25 21:46:04:471] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock
[2020-08-25 21:46:05:691] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #9
[2020-08-25 21:46:05:691] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: addBlock took 1220ms
[2020-08-25 21:46:05:693] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1679)]pipeline = [DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]]
[2020-08-25 21:46:05:693] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:255)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 21:46:05:695] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:267)]Send buf size 131768
[2020-08-25 21:46:05:695] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-25 21:46:05:729] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761834_21128 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 26445
[2020-08-25 21:46:05:784] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 2158112
[2020-08-25 21:46:05:786] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761834_21128 sending packet packet seqno: 1 offsetInBlock: 26445 lastPacketInBlock: true lastByteOffsetInBlock: 26445
[2020-08-25 21:46:05:794] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 6425075
[2020-08-25 21:46:05:794] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:622)]Closing old block BP-972878040-192.168.163.71-1586355896590:blk_1073761834_21128
[2020-08-25 21:46:05:796] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (397639453) connection to /192.168.163.72:8020 from root sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete
[2020-08-25 21:46:05:821] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (397639453) connection to /192.168.163.72:8020 from root got value #10
[2020-08-25 21:46:05:821] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: complete took 25ms
[2020-08-25 21:46:05:824] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@3ed96f41
[2020-08-25 21:46:05:824] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@3ed96f41
[2020-08-25 21:46:05:824] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@3ed96f41
[2020-08-25 21:46:05:825] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@3ed96f41
[2020-08-25 21:46:05:825] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-25 21:46:05:825] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (397639453) connection to /192.168.163.72:8020 from root: closed
[2020-08-25 21:46:05:825] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (397639453) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-25 21:46:05:929] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-25 21:48:11:718] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.guangda.shellUtil.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.guangda.shellUtil.MergeFilePath.<init>(MergeFilePath.java:20)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:107)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 21:48:11:756] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 21:48:11:757] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 21:48:11:914] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 21:48:11:929] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 21:48:11:931] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 21:48:11:932] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 21:48:11:934] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 21:48:11:937] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 21:48:12:699] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 21:48:12:711] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 21:48:12:719] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 21:48:12:722] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 21:48:12:722] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 21:48:12:730] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 21:48:12:732] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 21:48:12:735] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 21:48:13:483] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 21:48:13:494] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 21:48:13:496] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 21:48:13:498] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 21:48:13:499] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 21:48:13:500] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 21:48:13:502] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 21:48:14:228] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-25 21:48:14:257] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-25 21:48:15:768] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 21:48:15:769] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 21:48:15:770] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 21:48:15:771] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 21:48:15:813] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-25 21:48:15:873] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-25 21:48:15:875] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 21:48:15:876] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 21:48:15:876] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 21:48:15:877] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 21:48:15:910] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 21:48:15:998] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@49e400ff
[2020-08-25 21:48:16:032] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@1090df6f
[2020-08-25 21:48:17:381] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-25 21:48:17:398] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-25 21:50:26:370] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 21:50:26:376] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-25 21:50:26:539] [WARN ] [method:org.apache.hadoop.ipc.Client$Connection.handleConnectionFailure(Client.java:920)]Failed to connect to server: 192.168.163.71/192.168.163.71:8020: try once and fail.
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:597)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1698)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1682)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:897)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:112)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:961)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.Globber.listStatus(Globber.java:76)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:234)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.guangda.shellUtil.MergeFilePath.getPatternFile(MergeFilePath.java:25)
	at com.guangda.shellUtil.MergeFilePath.merge(MergeFilePath.java:67)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:111)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 21:50:26:552] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1244)]closing ipc connection to 192.168.163.71/192.168.163.71:8020: Connection refused
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:597)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1698)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1682)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:897)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:112)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:961)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.Globber.listStatus(Globber.java:76)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:234)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.guangda.shellUtil.MergeFilePath.getPatternFile(MergeFilePath.java:25)
	at com.guangda.shellUtil.MergeFilePath.merge(MergeFilePath.java:67)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:111)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-25 21:50:26:560] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (549956975) connection to /192.168.163.71:8020 from root: closed
[2020-08-25 21:50:26:584] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getListing over /192.168.163.71:8020. Trying to failover immediately.
java.net.ConnectException: Call From localhost/127.0.0.1 to 192.168.163.71:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:597)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1698)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1682)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:897)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:112)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:961)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:958)
	at org.apache.hadoop.fs.Globber.listStatus(Globber.java:76)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:234)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.guangda.shellUtil.MergeFilePath.getPatternFile(MergeFilePath.java:25)
	at com.guangda.shellUtil.MergeFilePath.merge(MergeFilePath.java:67)
	at com.guangda.shellUtil.MergeUtil.lambda$merge$0(MergeUtil.java:111)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	... 33 more
[2020-08-25 21:50:26:596] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 21:50:26:598] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@1090df6f
[2020-08-25 21:50:26:600] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 21:50:26:602] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-25 21:50:26:627] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (549956975) connection to /192.168.163.72:8020 from root: starting, having connections 1
[2020-08-25 21:50:26:738] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 21:50:26:749] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #0
[2020-08-25 21:50:26:750] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 150ms
[2020-08-25 21:50:26:842] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 21:50:26:846] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #1
[2020-08-25 21:50:26:846] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 4ms
[2020-08-25 21:50:26:849] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 21:50:26:851] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #2
[2020-08-25 21:50:26:851] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-25 21:50:26:854] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 21:50:26:856] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #3
[2020-08-25 21:50:26:856] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-25 21:50:26:863] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1273)]/megerlittlefile2/test08/part-m-00000: masked=rw-r--r--
[2020-08-25 21:50:26:927] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
[2020-08-25 21:50:26:950] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #4
[2020-08-25 21:50:26:951] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: create took 25ms
[2020-08-25 21:50:26:982] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.computePacketChunkSize(DFSOutputStream.java:389)]computePacketChunkSize: src=/megerlittlefile2/test08/part-m-00000, chunkSize=516, chunksPerPacket=126, packetSize=65016
[2020-08-25 21:50:27:046] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)]Lease renewer daemon for [DFSClient_NONMAPREDUCE_1838974737_13] with renew id 1 started
[2020-08-25 21:50:27:068] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 21:50:27:071] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #5
[2020-08-25 21:50:27:071] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 3ms
[2020-08-25 21:50:27:106] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 21:50:27:115] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 21:50:27:138] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
[2020-08-25 21:50:27:139] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #6
[2020-08-25 21:50:27:140] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getServerDefaults took 2ms
[2020-08-25 21:50:27:159] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-25 21:50:27:345] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:418)]DFSClient writeChunk allocating new packet seqno=0, src=/megerlittlefile2/test08/part-m-00000, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
[2020-08-25 21:50:27:354] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 21:50:27:356] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #7
[2020-08-25 21:50:27:356] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 2ms
[2020-08-25 21:50:27:359] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 21:50:27:360] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 21:50:27:368] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-25 21:50:27:371] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #8
[2020-08-25 21:50:27:371] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 3ms
[2020-08-25 21:50:27:374] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-25 21:50:27:375] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-25 21:50:27:383] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 0
[2020-08-25 21:50:27:384] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 1
[2020-08-25 21:50:27:385] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:867)]Waiting for ack for: 1
[2020-08-25 21:50:27:385] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:703)]Allocating new block
[2020-08-25 21:50:27:395] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock
[2020-08-25 21:50:27:402] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #9
[2020-08-25 21:50:27:402] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: addBlock took 7ms
[2020-08-25 21:50:27:405] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1679)]pipeline = [DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]
[2020-08-25 21:50:27:405] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:255)]Connecting to datanode 192.168.163.74:50010
[2020-08-25 21:50:27:408] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:267)]Send buf size 131768
[2020-08-25 21:50:27:408] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.74, datanodeId = DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]
[2020-08-25 21:50:27:455] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761835_21129 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 26445
[2020-08-25 21:50:27:534] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 3265033
[2020-08-25 21:50:27:536] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761835_21129 sending packet packet seqno: 1 offsetInBlock: 26445 lastPacketInBlock: true lastByteOffsetInBlock: 26445
[2020-08-25 21:50:27:550] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 1241718
[2020-08-25 21:50:27:550] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:622)]Closing old block BP-972878040-192.168.163.71-1586355896590:blk_1073761835_21129
[2020-08-25 21:50:27:555] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (549956975) connection to /192.168.163.72:8020 from root sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete
[2020-08-25 21:50:27:565] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (549956975) connection to /192.168.163.72:8020 from root got value #10
[2020-08-25 21:50:27:566] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: complete took 12ms
[2020-08-25 21:54:25:620] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (549956975) connection to /192.168.163.72:8020 from root: closed
[2020-08-25 21:54:29:213] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (549956975) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-25 21:54:29:217] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-25 21:54:29:219] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:473)]Lease renewer daemon for [] with renew id 1 expired
[2020-08-25 21:54:29:221] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:318)]Lease renewer daemon for [] with renew id 1 exited
[2020-08-25 21:54:29:224] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@1090df6f
[2020-08-25 21:54:29:226] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@1090df6f
[2020-08-25 21:54:29:227] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@1090df6f
[2020-08-25 21:54:29:229] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@1090df6f
[2020-08-25 21:54:29:231] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-25 21:54:29:233] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-25 22:17:34:561] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.guangda.shellUtil.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.guangda.shellUtil.MergeFilePath.<init>(MergeFilePath.java:30)
	at com.guangda.shellUtil.MergeUtil.merge(MergeUtil.java:58)
	at com.guangda.mergeFile.DemoTest.main(DemoTest.java:39)
[2020-08-25 22:17:34:629] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-25 22:17:34:630] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-25 22:17:34:696] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-25 22:17:34:707] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-25 22:17:34:707] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-25 22:17:34:708] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-25 22:17:34:708] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-25 22:17:34:710] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-25 22:17:34:820] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-25 22:17:34:826] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-25 22:17:34:831] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-25 22:17:34:833] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-25 22:17:34:833] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-25 22:17:34:834] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-25 22:17:34:841] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-25 22:17:34:842] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-25 22:17:34:957] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-25 22:17:34:974] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-25 22:17:34:977] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-25 22:17:34:980] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-25 22:17:34:982] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-25 22:17:34:983] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-25 22:17:34:984] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-25 22:17:35:058] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-25 22:17:35:063] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-25 22:17:35:339] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 22:17:35:340] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 22:17:35:340] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 22:17:35:340] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 22:17:35:348] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-25 22:17:35:362] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-25 22:17:35:363] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-25 22:17:35:363] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-25 22:17:35:363] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-25 22:17:35:363] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-25 22:17:35:372] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 22:17:35:387] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@130c12b7
[2020-08-25 22:17:35:398] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-25 22:17:35:713] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-25 22:17:35:718] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-25 22:17:35:740] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 22:17:35:741] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-25 22:17:35:785] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-25 22:17:35:787] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-25 22:17:35:822] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root got value #0
[2020-08-25 22:17:35:826] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.guangda.shellUtil.MergeFilePath.getPatternFile(MergeFilePath.java:36)
	at com.guangda.shellUtil.MergeFilePath.merge(MergeFilePath.java:81)
	at com.guangda.shellUtil.MergeUtil.merge(MergeUtil.java:61)
	at com.guangda.mergeFile.DemoTest.main(DemoTest.java:39)
[2020-08-25 22:17:35:827] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-25 22:17:35:827] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-25 22:17:35:828] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-25 22:17:35:828] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-25 22:17:35:829] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1833848849) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-25 22:17:35:829] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1833848849) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-25 22:17:35:834] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1833848849) connection to /192.168.163.72:8020 from root got value #0
[2020-08-25 22:17:35:835] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 7ms
[2020-08-25 22:17:35:858] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1833848849) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-25 22:17:35:861] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1833848849) connection to /192.168.163.72:8020 from root got value #1
[2020-08-25 22:17:35:861] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-25 22:17:45:791] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: closed
[2020-08-25 22:17:45:792] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-25 22:17:45:862] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1833848849) connection to /192.168.163.72:8020 from root: closed
[2020-08-25 22:17:45:863] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1833848849) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-25 22:20:36:018] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-25 22:20:36:022] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-25 22:20:36:022] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-25 22:20:36:023] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-25 22:20:36:023] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-25 22:20:36:024] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
