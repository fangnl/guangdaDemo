[2020-08-27 09:45:06:239] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 09:45:06:249] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 09:45:06:250] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 09:45:06:306] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-27 09:45:06:316] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-27 09:45:06:317] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-27 09:45:06:318] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-27 09:45:06:320] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-27 09:45:06:322] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 09:45:06:421] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 09:45:06:427] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 09:45:06:431] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 09:45:06:432] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 09:45:06:434] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 09:45:06:440] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 09:45:06:442] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 09:45:06:444] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 09:45:06:576] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 09:45:06:593] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 09:45:06:594] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 09:45:06:597] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 09:45:06:597] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 09:45:06:598] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 09:45:06:599] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 09:45:06:711] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 09:45:06:717] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 09:45:07:078] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 09:45:07:078] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 09:45:07:078] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 09:45:07:079] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 09:45:07:088] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 09:45:07:101] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 09:45:07:102] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 09:45:07:102] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 09:45:07:102] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 09:45:07:102] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 09:45:07:114] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 09:45:07:131] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@5a3dff7c
[2020-08-27 09:45:07:141] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@4683d820
[2020-08-27 09:45:07:456] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 09:45:07:461] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 09:45:07:488] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 09:45:07:489] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 09:45:07:533] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2099479325) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 09:45:07:535] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2099479325) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 09:45:07:620] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2099479325) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 09:45:07:621] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 151ms
[2020-08-27 09:45:07:661] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2099479325) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 09:45:07:677] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2099479325) connection to /192.168.163.71:8020 from root got value #1
[2020-08-27 09:45:07:678] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 17ms
[2020-08-27 09:45:09:890] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@4683d820
[2020-08-27 09:45:09:891] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@4683d820
[2020-08-27 09:45:09:891] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@4683d820
[2020-08-27 09:45:09:891] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 09:45:09:895] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2099479325) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 09:45:09:896] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2099479325) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-27 09:45:09:995] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 09:46:10:044] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 09:46:10:052] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 09:46:10:052] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 09:46:10:082] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-27 09:46:10:086] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-27 09:46:10:086] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-27 09:46:10:087] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-27 09:46:10:087] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-27 09:46:10:088] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 09:46:10:128] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 09:46:10:131] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 09:46:10:132] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 09:46:10:133] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 09:46:10:134] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 09:46:10:136] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 09:46:10:137] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 09:46:10:137] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 09:46:10:203] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 09:46:10:211] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 09:46:10:212] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 09:46:10:213] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 09:46:10:213] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 09:46:10:214] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 09:46:10:215] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 09:46:10:263] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 09:46:10:267] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 09:46:10:469] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 09:46:10:469] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 09:46:10:469] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 09:46:10:469] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 09:46:10:478] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 09:46:10:493] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 09:46:10:494] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 09:46:10:494] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 09:46:10:495] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 09:46:10:495] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 09:46:10:504] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 09:46:10:520] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@3d005476
[2020-08-27 09:46:10:531] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 09:46:10:754] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 09:46:10:759] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 09:46:10:790] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 09:46:10:792] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 09:46:10:841] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 09:46:10:844] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 09:46:10:853] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 09:46:10:853] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 81ms
[2020-08-27 09:46:10:872] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 09:46:10:874] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.71:8020 from root got value #1
[2020-08-27 09:46:10:875] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 4ms
[2020-08-27 09:46:20:878] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 09:46:20:878] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-27 09:48:47:052] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 09:48:47:053] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 09:48:47:053] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 09:48:47:053] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 09:48:47:054] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 09:57:45:164] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 09:57:45:174] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 09:57:45:174] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 09:57:45:209] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-27 09:57:45:212] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-27 09:57:45:213] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-27 09:57:45:213] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-27 09:57:45:214] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-27 09:57:45:214] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 09:57:45:253] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 09:57:45:257] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 09:57:45:258] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 09:57:45:259] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 09:57:45:261] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 09:57:45:268] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 09:57:45:269] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 09:57:45:270] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 09:57:45:379] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 09:57:45:387] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 09:57:45:388] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 09:57:45:389] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 09:57:45:389] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 09:57:45:390] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 09:57:45:390] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 09:57:45:435] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 09:57:45:439] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 09:57:45:656] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 09:57:45:656] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 09:57:45:657] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 09:57:45:657] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 09:57:45:657] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 09:57:45:657] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 09:57:45:657] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 09:57:45:658] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 09:57:45:664] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 09:57:45:664] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 09:57:45:676] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 09:57:45:676] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 09:57:45:677] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 09:57:45:677] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 09:57:45:677] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 09:57:45:677] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 09:57:45:677] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 09:57:45:677] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 09:57:45:678] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 09:57:45:677] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 09:57:45:685] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 09:57:45:685] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 09:57:45:696] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@2e269032
[2020-08-27 09:57:45:705] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@118a0de0
[2020-08-27 09:57:45:705] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@118a0de0
[2020-08-27 09:57:45:981] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 09:57:45:988] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 09:57:45:989] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 09:57:45:999] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@118a0de0
[2020-08-27 09:57:46:024] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 09:57:46:024] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 09:57:46:025] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 09:57:46:073] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 09:57:46:074] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 09:57:46:079] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 09:57:46:085] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root got value #1
[2020-08-27 09:57:46:086] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 78ms
[2020-08-27 09:57:46:086] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 09:57:46:086] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 78ms
[2020-08-27 09:57:46:107] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 09:57:46:107] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 09:57:46:108] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root got value #2
[2020-08-27 09:57:46:108] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-27 09:57:46:108] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root got value #3
[2020-08-27 09:57:46:109] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-27 09:57:56:117] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 09:57:56:118] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1131553735) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-27 10:01:47:334] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@118a0de0
[2020-08-27 10:01:47:336] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@118a0de0
[2020-08-27 10:01:47:336] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@118a0de0
[2020-08-27 10:01:47:336] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 10:01:47:337] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 11:00:48:159] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 11:00:48:171] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 11:00:48:171] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 11:00:48:222] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-27 11:00:48:229] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-27 11:00:48:229] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-27 11:00:48:230] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-27 11:00:48:231] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-27 11:00:48:233] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 11:00:48:309] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 11:00:48:314] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 11:00:48:317] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 11:00:48:318] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 11:00:48:320] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 11:00:48:324] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 11:00:48:326] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 11:00:48:327] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 11:00:48:412] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 11:00:48:424] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 11:00:48:425] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 11:00:48:426] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 11:00:48:427] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 11:00:48:427] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 11:00:48:428] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 11:00:48:488] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 11:00:48:492] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 11:00:48:747] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:00:48:747] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:00:48:747] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:00:48:748] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:00:48:747] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:00:48:748] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:00:48:748] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:00:48:748] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:00:48:757] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 11:00:48:757] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 11:00:48:770] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 11:00:48:770] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 11:00:48:771] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:00:48:771] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:00:48:771] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:00:48:771] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:00:48:771] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:00:48:772] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:00:48:772] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:00:48:772] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:00:48:780] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 11:00:48:780] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 11:00:48:793] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@6e6e3dc4
[2020-08-27 11:00:48:802] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6288a878
[2020-08-27 11:00:48:802] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6288a878
[2020-08-27 11:00:49:124] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 11:00:49:131] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 11:00:49:131] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 11:00:49:138] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6288a878
[2020-08-27 11:00:49:158] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:00:49:158] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:00:49:159] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 11:00:49:201] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2069934188) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 11:00:49:203] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2069934188) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:00:49:207] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2069934188) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:00:49:247] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2069934188) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 11:00:49:248] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2069934188) connection to /192.168.163.71:8020 from root got value #1
[2020-08-27 11:00:49:251] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 11:00:49:251] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 11:00:49:252] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 11:00:49:253] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6288a878
[2020-08-27 11:00:49:254] [WARN ] [method:org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor.failover(RetryInvocationHandler.java:218)]A failover has occurred since the start of call #0 ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.72:8020
[2020-08-27 11:00:49:254] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:00:49:254] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:00:49:254] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-27 11:00:49:256] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-27 11:00:49:256] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:00:49:256] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:00:49:324] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root got value #1
[2020-08-27 11:00:49:324] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 70ms
[2020-08-27 11:00:49:325] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root got value #0
[2020-08-27 11:00:49:325] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 71ms
[2020-08-27 11:00:49:361] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 11:00:49:363] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 11:00:49:375] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root got value #3
[2020-08-27 11:00:49:376] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 15ms
[2020-08-27 11:00:49:379] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root got value #2
[2020-08-27 11:00:49:380] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 18ms
[2020-08-27 11:00:59:256] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2069934188) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 11:00:59:256] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2069934188) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-27 11:00:59:382] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root: closed
[2020-08-27 11:00:59:382] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2069934188) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-27 11:03:31:451] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6288a878
[2020-08-27 11:03:31:452] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6288a878
[2020-08-27 11:03:31:453] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@6288a878
[2020-08-27 11:03:31:453] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@6288a878
[2020-08-27 11:03:31:453] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 11:03:31:454] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 11:07:38:070] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 11:07:38:083] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 11:07:38:083] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 11:07:38:141] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-27 11:07:38:147] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-27 11:07:38:147] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-27 11:07:38:148] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-27 11:07:38:148] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-27 11:07:38:149] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 11:07:38:225] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 11:07:38:229] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 11:07:38:231] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 11:07:38:232] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 11:07:38:234] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 11:07:38:236] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 11:07:38:237] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 11:07:38:238] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 11:07:38:305] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 11:07:38:315] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 11:07:38:316] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 11:07:38:317] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 11:07:38:317] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 11:07:38:317] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 11:07:38:318] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 11:07:38:371] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 11:07:38:376] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 11:07:38:607] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:07:38:607] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:07:38:607] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:07:38:608] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:07:38:607] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:07:38:608] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:07:38:608] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:07:38:609] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:07:38:622] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 11:07:38:622] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 11:07:38:637] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 11:07:38:637] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 11:07:38:638] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:07:38:638] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:07:38:638] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:07:38:639] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:07:38:639] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:07:38:639] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:07:38:638] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:07:38:639] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:07:38:651] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 11:07:38:651] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 11:07:38:666] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@47b793f6
[2020-08-27 11:07:38:675] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@3de88811
[2020-08-27 11:07:38:676] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@3de88811
[2020-08-27 11:07:38:972] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 11:07:38:976] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 11:07:38:976] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 11:07:38:982] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@3de88811
[2020-08-27 11:07:39:001] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:07:39:001] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:07:39:002] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 11:07:39:039] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1720335117) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 11:07:39:041] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1720335117) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:07:39:045] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1720335117) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:07:39:049] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1720335117) connection to /192.168.163.71:8020 from root got value #1
[2020-08-27 11:07:39:050] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1720335117) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 11:07:39:054] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 11:07:39:054] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 11:07:39:055] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 11:07:39:056] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@3de88811
[2020-08-27 11:07:39:056] [WARN ] [method:org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor.failover(RetryInvocationHandler.java:218)]A failover has occurred since the start of call #0 ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.72:8020
[2020-08-27 11:07:39:056] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:07:39:056] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:07:39:057] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-27 11:07:39:058] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-27 11:07:39:058] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:07:39:058] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:07:39:059] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root got value #0
[2020-08-27 11:07:39:060] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root got value #1
[2020-08-27 11:07:39:060] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 4ms
[2020-08-27 11:07:39:060] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 4ms
[2020-08-27 11:07:39:081] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 11:07:39:082] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 11:07:39:082] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root got value #3
[2020-08-27 11:07:39:083] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-27 11:07:39:085] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root got value #2
[2020-08-27 11:07:39:085] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 4ms
[2020-08-27 11:07:49:058] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1720335117) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 11:07:49:058] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1720335117) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-27 11:07:49:085] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root: closed
[2020-08-27 11:07:49:085] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1720335117) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-27 11:10:11:681] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@3de88811
[2020-08-27 11:10:11:682] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@3de88811
[2020-08-27 11:10:11:682] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@3de88811
[2020-08-27 11:10:11:682] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@3de88811
[2020-08-27 11:10:11:682] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 11:10:11:683] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 11:11:51:171] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 11:11:51:182] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 11:11:51:182] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 11:11:51:218] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-27 11:11:51:222] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-27 11:11:51:224] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-27 11:11:51:225] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-27 11:11:51:225] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-27 11:11:51:226] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 11:11:51:282] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 11:11:51:285] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 11:11:51:288] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 11:11:51:289] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 11:11:51:290] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 11:11:51:290] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 11:11:51:292] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 11:11:51:293] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 11:11:51:374] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 11:11:51:383] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 11:11:51:384] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 11:11:51:385] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 11:11:51:385] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 11:11:51:385] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 11:11:51:386] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 11:11:51:428] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 11:11:51:431] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 11:11:51:618] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:11:51:619] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:11:51:619] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:11:51:619] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:11:51:618] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:11:51:620] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:11:51:620] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:11:51:620] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:11:51:627] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 11:11:51:627] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 11:11:51:637] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 11:11:51:637] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 11:11:51:638] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:11:51:638] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:11:51:638] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:11:51:638] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 11:11:51:639] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 11:11:51:639] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 11:11:51:639] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:11:51:639] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 11:11:51:648] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 11:11:51:648] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 11:11:51:661] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@6486e19
[2020-08-27 11:11:51:671] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@52a5ae27
[2020-08-27 11:11:51:672] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@52a5ae27
[2020-08-27 11:11:51:896] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 11:11:51:902] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 11:11:51:902] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 11:11:51:911] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@52a5ae27
[2020-08-27 11:11:51:935] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:11:51:935] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:11:51:937] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 11:11:51:982] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (92078980) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 11:11:51:983] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (92078980) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:11:51:988] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (92078980) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:11:51:993] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (92078980) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 11:11:51:993] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (92078980) connection to /192.168.163.71:8020 from root got value #1
[2020-08-27 11:11:51:997] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 11:11:51:997] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 11:11:51:998] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 11:11:51:999] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@52a5ae27
[2020-08-27 11:11:51:999] [WARN ] [method:org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor.failover(RetryInvocationHandler.java:218)]A failover has occurred since the start of call #1 ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.72:8020
[2020-08-27 11:11:51:999] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:11:51:999] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 11:11:52:000] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-27 11:11:52:001] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (92078980) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-27 11:11:52:001] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (92078980) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:11:52:002] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (92078980) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 11:11:52:003] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (92078980) connection to /192.168.163.72:8020 from root got value #0
[2020-08-27 11:11:52:003] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (92078980) connection to /192.168.163.72:8020 from root got value #1
[2020-08-27 11:11:52:003] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 4ms
[2020-08-27 11:11:52:003] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 4ms
[2020-08-27 11:11:52:022] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (92078980) connection to /192.168.163.72:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 11:11:52:023] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (92078980) connection to /192.168.163.72:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 11:11:52:024] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (92078980) connection to /192.168.163.72:8020 from root got value #3
[2020-08-27 11:11:52:024] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-27 11:11:52:026] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (92078980) connection to /192.168.163.72:8020 from root got value #2
[2020-08-27 11:11:52:026] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 4ms
[2020-08-27 11:11:55:339] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@52a5ae27
[2020-08-27 11:11:55:339] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@52a5ae27
[2020-08-27 11:11:55:339] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@52a5ae27
[2020-08-27 11:11:55:339] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@52a5ae27
[2020-08-27 11:11:55:339] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 11:11:55:343] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (92078980) connection to /192.168.163.72:8020 from root: closed
[2020-08-27 11:11:55:343] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (92078980) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-27 11:11:55:344] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (92078980) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 11:11:55:344] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (92078980) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-27 11:11:55:444] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 14:57:53:493] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 14:57:53:514] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 14:57:53:515] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 14:57:53:588] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-27 14:57:53:597] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-27 14:57:53:598] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-27 14:57:53:599] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-27 14:57:53:600] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-27 14:57:53:602] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 14:57:53:691] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 14:57:53:698] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 14:57:53:703] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 14:57:53:704] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 14:57:53:708] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 14:57:53:713] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 14:57:53:716] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 14:57:53:717] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 14:57:53:854] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 14:57:53:863] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 14:57:53:863] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 14:57:53:865] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 14:57:53:865] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 14:57:53:865] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 14:57:53:866] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 14:57:53:923] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 14:57:53:931] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 14:57:54:189] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 14:57:54:189] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 14:57:54:189] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 14:57:54:189] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 14:57:54:197] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 14:57:54:210] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 14:57:54:210] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 14:57:54:211] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 14:57:54:211] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 14:57:54:211] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 14:57:54:224] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 14:57:54:240] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@3d005476
[2020-08-27 14:57:54:251] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 14:57:54:562] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 14:57:54:567] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 14:57:54:594] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 14:57:54:594] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 14:57:54:640] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 14:57:54:641] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 14:57:54:651] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 14:57:54:655] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 14:57:54:656] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 14:57:54:656] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 14:57:54:657] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 14:57:54:657] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-27 14:57:54:658] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (624280516) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-27 14:57:54:658] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 14:57:54:659] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.72:8020 from root got value #0
[2020-08-27 14:57:54:660] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-27 14:57:54:683] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 14:57:54:685] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.72:8020 from root got value #1
[2020-08-27 14:57:54:685] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-27 14:58:04:645] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 14:58:04:646] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-27 14:58:04:686] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (624280516) connection to /192.168.163.72:8020 from root: closed
[2020-08-27 14:58:04:687] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (624280516) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-27 14:59:57:548] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 14:59:57:548] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 14:59:57:548] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 14:59:57:549] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 14:59:57:549] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 14:59:57:549] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 15:04:55:059] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 15:04:55:071] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 15:04:55:071] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 15:04:55:125] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-27 15:04:55:132] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-27 15:04:55:132] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-27 15:04:55:132] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-27 15:04:55:133] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-27 15:04:55:134] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 15:04:55:201] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 15:04:55:206] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 15:04:55:209] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 15:04:55:209] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 15:04:55:211] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 15:04:55:214] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 15:04:55:215] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 15:04:55:216] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 15:04:55:305] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 15:04:55:315] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 15:04:55:316] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 15:04:55:318] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 15:04:55:318] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 15:04:55:318] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 15:04:55:319] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 15:04:55:372] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 15:04:55:377] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 15:04:55:593] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:04:55:593] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:04:55:593] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:04:55:593] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:04:55:603] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 15:04:55:618] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 15:04:55:618] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:04:55:619] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:04:55:619] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:04:55:619] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:04:55:631] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 15:04:55:646] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@3d005476
[2020-08-27 15:04:55:654] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:04:55:933] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 15:04:55:938] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 15:04:55:967] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:04:55:968] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 15:04:56:009] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 15:04:56:010] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:04:56:018] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 15:04:56:022] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 15:04:56:023] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 15:04:56:023] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:04:56:023] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:04:56:023] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-27 15:04:56:025] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (624280516) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-27 15:04:56:025] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:04:56:026] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.72:8020 from root got value #0
[2020-08-27 15:04:56:026] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-27 15:04:56:045] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 15:04:56:046] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.72:8020 from root got value #1
[2020-08-27 15:04:56:046] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-27 15:05:06:018] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 15:05:06:021] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-27 15:05:06:049] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (624280516) connection to /192.168.163.72:8020 from root: closed
[2020-08-27 15:05:06:049] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (624280516) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-27 15:06:55:781] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:06:55:781] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:06:55:781] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:06:55:781] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:06:55:781] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 15:06:55:782] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 15:08:17:405] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 15:08:17:417] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 15:08:17:418] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 15:08:17:459] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-27 15:08:17:465] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-27 15:08:17:466] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-27 15:08:17:466] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-27 15:08:17:466] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-27 15:08:17:467] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 15:08:17:525] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 15:08:17:529] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 15:08:17:531] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 15:08:17:532] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 15:08:17:533] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 15:08:17:538] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 15:08:17:539] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 15:08:17:540] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 15:08:17:612] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 15:08:17:621] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 15:08:17:622] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 15:08:17:623] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 15:08:17:623] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 15:08:17:623] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 15:08:17:624] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 15:08:17:665] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 15:08:17:669] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 15:08:17:918] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:08:17:919] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:08:17:919] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:08:17:919] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:08:17:918] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:08:17:919] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:08:17:920] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:08:17:920] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:08:17:928] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 15:08:17:928] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 15:08:17:941] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 15:08:17:941] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 15:08:17:941] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:08:17:942] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:08:17:942] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:08:17:942] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:08:17:942] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:08:17:942] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:08:17:942] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:08:17:943] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:08:17:954] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 15:08:17:954] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 15:08:17:967] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@f8ceb61
[2020-08-27 15:08:17:976] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@13445c65
[2020-08-27 15:08:17:976] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@13445c65
[2020-08-27 15:08:18:264] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 15:08:18:270] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 15:08:18:270] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 15:08:18:277] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@13445c65
[2020-08-27 15:08:18:298] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:08:18:298] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:08:18:299] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 15:08:18:338] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2068123306) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 15:08:18:340] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2068123306) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:08:18:344] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2068123306) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:08:18:348] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2068123306) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 15:08:18:348] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2068123306) connection to /192.168.163.71:8020 from root got value #1
[2020-08-27 15:08:18:352] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 15:08:18:352] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 15:08:18:353] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 15:08:18:354] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@13445c65
[2020-08-27 15:08:18:354] [WARN ] [method:org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor.failover(RetryInvocationHandler.java:218)]A failover has occurred since the start of call #1 ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.72:8020
[2020-08-27 15:08:18:354] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:08:18:355] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:08:18:355] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-27 15:08:18:356] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-27 15:08:18:357] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:08:18:357] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:08:18:358] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root got value #0
[2020-08-27 15:08:18:358] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 4ms
[2020-08-27 15:08:18:358] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root got value #1
[2020-08-27 15:08:18:359] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 5ms
[2020-08-27 15:08:18:377] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 15:08:18:378] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 15:08:18:379] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root got value #2
[2020-08-27 15:08:18:379] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-27 15:08:18:379] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root got value #3
[2020-08-27 15:08:18:379] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-27 15:08:28:355] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2068123306) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 15:08:28:356] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2068123306) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-27 15:08:28:383] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root: closed
[2020-08-27 15:08:28:383] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2068123306) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-27 15:10:41:292] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@13445c65
[2020-08-27 15:10:41:293] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@13445c65
[2020-08-27 15:10:41:293] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@13445c65
[2020-08-27 15:10:41:293] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@13445c65
[2020-08-27 15:10:41:293] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 15:10:41:294] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 15:39:11:263] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 15:39:11:274] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 15:39:11:275] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 15:39:11:325] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-27 15:39:11:334] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-27 15:39:11:334] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-27 15:39:11:335] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-27 15:39:11:335] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-27 15:39:11:337] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 15:39:11:423] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 15:39:11:428] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 15:39:11:430] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 15:39:11:431] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 15:39:11:433] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 15:39:11:435] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 15:39:11:436] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 15:39:11:437] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 15:39:11:542] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 15:39:11:555] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 15:39:11:556] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 15:39:11:558] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 15:39:11:559] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 15:39:11:559] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 15:39:11:560] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 15:39:11:657] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 15:39:11:660] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 15:39:11:925] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:39:11:926] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:39:11:926] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:39:11:926] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:39:11:936] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 15:39:11:953] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 15:39:11:954] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:39:11:954] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:39:11:954] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:39:11:954] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:39:11:972] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 15:39:11:991] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@3d005476
[2020-08-27 15:39:12:002] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:39:12:348] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 15:39:12:352] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 15:39:12:377] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:39:12:378] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 15:39:12:417] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 15:39:12:419] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:39:12:429] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 15:39:12:433] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 15:39:12:434] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 15:39:12:435] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:39:12:435] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:39:12:435] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-27 15:39:12:436] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (624280516) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-27 15:39:12:436] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:39:12:437] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.72:8020 from root got value #0
[2020-08-27 15:39:12:438] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-27 15:39:12:463] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (624280516) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 15:39:12:465] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (624280516) connection to /192.168.163.72:8020 from root got value #1
[2020-08-27 15:39:12:465] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-27 15:39:13:530] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:39:13:530] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:39:13:530] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:39:13:530] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@188d18f3
[2020-08-27 15:39:13:531] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 15:39:13:535] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (624280516) connection to /192.168.163.72:8020 from root: closed
[2020-08-27 15:39:13:535] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (624280516) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-27 15:39:13:535] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 15:39:13:536] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (624280516) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-27 15:39:13:636] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-27 15:59:50:026] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 15:59:50:038] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-27 15:59:50:039] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-27 15:59:50:084] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-27 15:59:50:088] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-27 15:59:50:089] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-27 15:59:50:089] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-27 15:59:50:090] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-27 15:59:50:091] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-27 15:59:50:167] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-27 15:59:50:171] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-27 15:59:50:174] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-27 15:59:50:174] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-27 15:59:50:176] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-27 15:59:50:178] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-27 15:59:50:179] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-27 15:59:50:180] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-27 15:59:50:249] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-27 15:59:50:256] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-27 15:59:50:257] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-27 15:59:50:258] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-27 15:59:50:258] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-27 15:59:50:258] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-27 15:59:50:259] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-27 15:59:50:298] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-27 15:59:50:303] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-27 15:59:50:539] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:59:50:539] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:59:50:539] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:59:50:539] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:59:50:549] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-27 15:59:50:562] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-27 15:59:50:563] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-27 15:59:50:563] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-27 15:59:50:563] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-27 15:59:50:563] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-27 15:59:50:575] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 15:59:50:589] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@4234bb63
[2020-08-27 15:59:50:598] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-27 15:59:50:922] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-27 15:59:50:927] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-27 15:59:50:954] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:59:50:954] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-27 15:59:50:997] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-27 15:59:50:999] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:59:51:007] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root got value #0
[2020-08-27 15:59:51:011] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-27 15:59:51:012] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-27 15:59:51:012] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-27 15:59:51:012] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-27 15:59:51:013] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-27 15:59:51:014] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-27 15:59:51:014] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-27 15:59:51:015] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root got value #0
[2020-08-27 15:59:51:015] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-27 15:59:51:036] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-27 15:59:51:038] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root got value #1
[2020-08-27 15:59:51:038] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-27 15:59:51:735] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-27 15:59:51:735] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-27 15:59:51:735] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-27 15:59:51:735] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-27 15:59:51:735] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-27 15:59:51:739] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root: closed
[2020-08-27 15:59:51:739] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-27 15:59:51:740] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root: closed
[2020-08-27 15:59:51:740] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-27 15:59:51:838] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
