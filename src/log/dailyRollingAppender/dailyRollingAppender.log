[2020-08-26 08:28:24:583] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.guangda.util.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.guangda.util.MergeFilePath.<init>(MergeFilePath.java:33)
	at com.guangda.util.MergeUtil.merge(MergeUtil.java:58)
	at com.guangda.mergeFile.DemoTest.main(DemoTest.java:34)
[2020-08-26 08:28:24:631] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 08:28:24:631] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 08:28:24:676] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 08:28:24:686] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 08:28:24:687] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 08:28:24:687] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 08:28:24:688] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 08:28:24:689] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 08:28:24:787] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 08:28:24:793] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 08:28:24:796] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 08:28:24:797] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 08:28:24:798] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 08:28:24:798] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 08:28:24:802] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 08:28:24:803] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 08:28:24:890] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 08:28:24:903] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 08:28:24:904] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 08:28:24:905] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 08:28:24:906] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 08:28:24:906] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 08:28:24:907] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 08:28:24:976] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 08:28:24:984] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 08:28:25:305] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 08:28:25:305] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 08:28:25:305] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 08:28:25:305] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 08:28:25:314] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 08:28:25:327] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-26 08:28:25:327] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 08:28:25:327] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 08:28:25:328] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 08:28:25:328] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 08:28:25:338] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 08:28:25:353] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@130c12b7
[2020-08-26 08:28:25:362] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 08:28:25:708] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 08:28:25:713] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 08:28:25:741] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 08:28:25:742] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 08:28:25:778] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 08:28:25:780] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 08:28:25:791] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 08:28:25:791] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 67ms
[2020-08-26 08:28:25:812] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 08:28:25:820] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root got value #1
[2020-08-26 08:28:25:820] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 8ms
[2020-08-26 08:28:27:113] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 08:28:27:114] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 08:28:27:114] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 08:28:27:114] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 08:28:27:117] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 08:28:27:118] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 08:28:27:219] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 08:29:51:237] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.guangda.util.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.guangda.util.MergeFilePath.<init>(MergeFilePath.java:33)
	at com.guangda.util.MergeUtil.merge(MergeUtil.java:58)
	at com.guangda.mergeFile.DemoTest.main(DemoTest.java:34)
[2020-08-26 08:29:51:279] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 08:29:51:279] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 08:29:51:311] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 08:29:51:318] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 08:29:51:319] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 08:29:51:319] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 08:29:51:319] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 08:29:51:320] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 08:29:51:364] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 08:29:51:367] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 08:29:51:369] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 08:29:51:370] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 08:29:51:370] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 08:29:51:370] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 08:29:51:373] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 08:29:51:374] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 08:29:51:462] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 08:29:51:472] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 08:29:51:473] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 08:29:51:474] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 08:29:51:474] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 08:29:51:474] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 08:29:51:475] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 08:29:51:508] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 08:29:51:512] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 08:29:51:755] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 08:29:51:756] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 08:29:51:756] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 08:29:51:756] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 08:29:51:764] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 08:29:51:777] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster:8020
[2020-08-26 08:29:51:777] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 08:29:51:778] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 08:29:51:778] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 08:29:51:778] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 08:29:51:786] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 08:29:51:800] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@130c12b7
[2020-08-26 08:29:51:812] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 08:29:52:093] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 08:29:52:097] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 08:29:52:128] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 08:29:52:129] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 08:29:52:174] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 08:29:52:176] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 08:29:52:192] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 08:29:52:192] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 85ms
[2020-08-26 08:29:52:218] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 08:29:52:221] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root got value #1
[2020-08-26 08:29:52:221] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-26 08:29:52:902] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 08:29:52:903] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 08:29:52:903] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 08:29:52:903] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 08:29:52:905] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 08:29:52:906] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 08:29:53:007] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 08:32:16:227] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.48.75 instead (on interface en7)
[2020-08-26 08:32:16:232] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Set SPARK_LOCAL_IP if you need to bind to another address
[2020-08-26 08:32:16:288] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Running Spark version 2.3.1
[2020-08-26 08:32:16:338] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 08:32:16:345] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 08:32:16:345] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 08:32:16:346] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 08:32:16:346] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 08:32:16:347] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 08:32:16:420] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1555)
	at org.apache.hadoop.security.SecurityUtil.getLogSlowLookupsEnabled(SecurityUtil.java:497)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:90)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:289)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:277)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:833)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:803)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:676)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2467)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2467)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:292)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.guangda.mergeFile.SparkMerge.merge(SparkMerge.java:28)
	at com.guangda.mergeFile.SparkMerge.main(SparkMerge.java:14)
[2020-08-26 08:32:16:506] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 08:32:16:507] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 08:32:16:571] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 08:32:16:579] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 08:32:16:582] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 08:32:16:583] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 08:32:16:585] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 08:32:16:586] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 08:32:16:586] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 08:32:16:587] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 08:32:16:664] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 08:32:16:669] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 08:32:16:670] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 08:32:16:671] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 08:32:16:671] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 08:32:16:671] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 08:32:16:672] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 08:32:16:684] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Submitted application: merge
[2020-08-26 08:32:16:786] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls to: fangnailiang,root
[2020-08-26 08:32:16:788] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls to: fangnailiang,root
[2020-08-26 08:32:16:790] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing view acls groups to: 
[2020-08-26 08:32:16:791] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Changing modify acls groups to: 
[2020-08-26 08:32:16:792] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fangnailiang, root); groups with view permissions: Set(); users  with modify permissions: Set(fangnailiang, root); groups with modify permissions: Set()
[2020-08-26 08:32:16:816] [DEBUG] [method:org.apache.spark.internal.Logging$class.logDebug(Logging.scala:58)]Created SSL options for fs: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[2020-08-26 08:32:16:886] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Using SLF4J as the default logging framework
[2020-08-26 08:32:16:889] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
[2020-08-26 08:32:16:889] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
[2020-08-26 08:32:16:911] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.eventLoopThreads: 24
[2020-08-26 08:32:16:935] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]Platform: MacOS
[2020-08-26 08:32:16:937] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noUnsafe: false
[2020-08-26 08:32:16:937] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]Java version: 8
[2020-08-26 08:32:16:939] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.theUnsafe: available
[2020-08-26 08:32:16:940] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]sun.misc.Unsafe.copyMemory: available
[2020-08-26 08:32:16:940] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.Buffer.address: available
[2020-08-26 08:32:16:941] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]direct buffer constructor: available
[2020-08-26 08:32:16:941] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.Bits.unaligned: available, true
[2020-08-26 08:32:16:941] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
[2020-08-26 08:32:16:942] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]java.nio.DirectByteBuffer.<init>(long, int): available
[2020-08-26 08:32:16:942] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]sun.misc.Unsafe: available
[2020-08-26 08:32:16:942] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.tmpdir: /var/folders/r3/pbs1kc5s4b53dx4mw86904gw0000gn/T (java.io.tmpdir)
[2020-08-26 08:32:16:942] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.bitMode: 64 (sun.arch.data.model)
[2020-08-26 08:32:16:944] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noPreferDirect: false
[2020-08-26 08:32:16:944] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxDirectMemory: 3817865216 bytes
[2020-08-26 08:32:16:944] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.uninitializedArrayAllocationThreshold: -1
[2020-08-26 08:32:16:945] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]java.nio.ByteBuffer.cleaner(): available
[2020-08-26 08:32:16:961] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.noKeySetOptimization: false
[2020-08-26 08:32:16:961] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.selectorAutoRebuildThreshold: 512
[2020-08-26 08:32:16:970] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)]org.jctools-core.MpscChunkedArrayQueue: available
[2020-08-26 08:32:17:008] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.level: simple
[2020-08-26 08:32:17:008] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]-Dio.netty.leakDetection.targetRecords: 4
[2020-08-26 08:32:17:012] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numHeapArenas: 24
[2020-08-26 08:32:17:012] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.numDirectArenas: 24
[2020-08-26 08:32:17:012] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.pageSize: 8192
[2020-08-26 08:32:17:012] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxOrder: 11
[2020-08-26 08:32:17:013] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.chunkSize: 16777216
[2020-08-26 08:32:17:013] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.tinyCacheSize: 512
[2020-08-26 08:32:17:013] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.smallCacheSize: 256
[2020-08-26 08:32:17:013] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.normalCacheSize: 64
[2020-08-26 08:32:17:013] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.maxCachedBufferCapacity: 32768
[2020-08-26 08:32:17:014] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.cacheTrimInterval: 8192
[2020-08-26 08:32:17:014] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.useCacheForAllThreads: true
[2020-08-26 08:32:17:117] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.processId: 77478 (auto-detected)
[2020-08-26 08:32:17:120] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv4Stack: false
[2020-08-26 08:32:17:120] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Djava.net.preferIPv6Addresses: false
[2020-08-26 08:32:17:123] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:86)]Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
[2020-08-26 08:32:17:124] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:81)]Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
[2020-08-26 08:32:17:129] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.machineId: 00:50:56:ff:fe:c0:00:08 (auto-detected)
[2020-08-26 08:32:17:156] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.allocator.type: pooled
[2020-08-26 08:32:17:156] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.threadLocalDirectBufferSize: 65536
[2020-08-26 08:32:17:156] [DEBUG] [method:io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:76)]-Dio.netty.maxThreadLocalCharBufferSize: 16384
[2020-08-26 08:32:17:169] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:172] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:174] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:175] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:177] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:178] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:180] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:181] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:182] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:184] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:185] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:186] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:188] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:189] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:190] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:192] [WARN ] [method:org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)]Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
[2020-08-26 08:32:17:196] [ERROR] [method:org.apache.spark.internal.Logging$class.logError(Logging.scala:91)]Error initializing SparkContext.
java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 08:32:17:203] [INFO ] [method:org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)]Successfully stopped SparkContext
[2020-08-26 10:00:25:694] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.guangda.util.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.guangda.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.guangda.util.MergeUtil.merge(MergeUtil.java:58)
	at com.guangda.mergeFile.DemoTest.main(DemoTest.java:34)
[2020-08-26 10:00:25:751] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 10:00:25:752] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 10:00:25:796] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 10:00:25:810] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 10:00:25:811] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 10:00:25:812] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 10:00:25:814] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 10:00:25:818] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 10:00:25:886] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 10:00:25:889] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 10:00:25:891] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 10:00:25:891] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 10:00:25:892] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 10:00:25:892] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 10:00:25:896] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 10:00:25:897] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 10:00:25:967] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 10:00:25:975] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 10:00:25:975] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 10:00:25:976] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 10:00:25:976] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 10:00:25:977] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 10:00:25:977] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 10:00:26:013] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 10:00:26:019] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 10:00:26:298] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:00:26:299] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:00:26:299] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:00:26:299] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:00:26:308] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 10:00:26:322] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 10:00:26:322] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:00:26:323] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:00:26:323] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:00:26:323] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:00:26:332] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 10:00:26:348] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@130c12b7
[2020-08-26 10:00:26:358] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 10:00:26:713] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 10:00:26:718] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 10:00:26:741] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 10:00:26:742] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 10:00:26:780] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 10:00:26:782] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 10:00:26:793] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 10:00:26:794] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 67ms
[2020-08-26 10:00:26:815] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:00:26:817] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root got value #1
[2020-08-26 10:00:26:818] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-26 10:00:27:698] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 10:00:27:698] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 10:00:27:698] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@1a6d8329
[2020-08-26 10:00:27:699] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 10:00:27:701] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 10:00:27:702] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1833848849) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 10:00:27:803] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 10:30:28:387] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:107)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 10:30:28:402] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 10:30:28:403] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 10:30:28:450] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 10:30:28:455] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 10:30:28:456] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 10:30:28:456] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 10:30:28:457] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 10:30:28:458] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 10:30:28:519] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 10:30:28:523] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 10:30:28:525] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 10:30:28:526] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 10:30:28:526] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 10:30:28:531] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 10:30:28:532] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 10:30:28:533] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 10:30:28:613] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 10:30:28:621] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 10:30:28:622] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 10:30:28:622] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 10:30:28:623] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 10:30:28:623] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 10:30:28:623] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 10:30:28:674] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 10:30:28:679] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 10:30:28:876] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:30:28:877] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:30:28:877] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:30:28:877] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:30:28:876] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:30:28:877] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:30:28:877] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:30:28:878] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:30:28:876] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:30:28:878] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:30:28:878] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:30:28:878] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:30:28:885] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 10:30:28:886] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 10:30:28:886] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 10:30:28:903] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 10:30:28:903] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 10:30:28:903] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 10:30:28:903] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:30:28:904] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:30:28:904] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:30:28:904] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:30:28:903] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:30:28:905] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:30:28:905] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:30:28:905] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:30:28:904] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:30:28:905] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:30:28:905] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:30:28:906] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:30:28:913] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 10:30:28:913] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 10:30:28:913] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 10:30:28:926] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@3823e641
[2020-08-26 10:30:28:936] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@53fd5efb
[2020-08-26 10:30:28:937] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@53fd5efb
[2020-08-26 10:30:28:937] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@53fd5efb
[2020-08-26 10:30:29:229] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 10:30:29:234] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 10:30:29:234] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 10:30:29:234] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 10:30:29:240] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@53fd5efb
[2020-08-26 10:30:29:240] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@53fd5efb
[2020-08-26 10:30:29:258] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 10:30:29:258] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 10:30:29:258] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 10:30:29:259] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 10:30:29:299] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 10:30:29:300] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:30:29:303] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 10:30:29:304] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 10:30:29:310] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #2
[2020-08-26 10:30:29:311] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 10:30:29:311] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 67ms
[2020-08-26 10:30:29:311] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 67ms
[2020-08-26 10:30:29:311] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #1
[2020-08-26 10:30:29:312] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 68ms
[2020-08-26 10:30:29:333] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:30:29:334] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:30:29:334] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:30:29:334] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #4
[2020-08-26 10:30:29:335] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 10:30:29:335] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #5
[2020-08-26 10:30:29:335] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 10:30:29:336] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #3
[2020-08-26 10:30:29:336] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:30:29:336] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-26 10:30:29:337] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #6
[2020-08-26 10:30:29:337] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 10:30:29:338] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:30:29:338] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #7
[2020-08-26 10:30:29:339] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 10:30:29:340] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1273)]/megerlittlefile2/test08/part-m-00000: masked=rw-r--r--
[2020-08-26 10:30:29:382] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
[2020-08-26 10:30:29:404] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #8
[2020-08-26 10:30:29:404] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: create took 22ms
[2020-08-26 10:30:29:414] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.computePacketChunkSize(DFSOutputStream.java:389)]computePacketChunkSize: src=/megerlittlefile2/test08/part-m-00000, chunkSize=516, chunksPerPacket=126, packetSize=65016
[2020-08-26 10:30:29:417] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)]Lease renewer daemon for [DFSClient_NONMAPREDUCE_-828200238_13] with renew id 1 started
[2020-08-26 10:30:29:424] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 10:30:29:444] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #9
[2020-08-26 10:30:29:445] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 22ms
[2020-08-26 10:30:29:460] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 10:30:29:463] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.74:50010
[2020-08-26 10:30:29:493] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
[2020-08-26 10:30:29:511] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #10
[2020-08-26 10:30:29:512] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getServerDefaults took 19ms
[2020-08-26 10:30:29:519] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.74, datanodeId = DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-26 10:30:29:621] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:418)]DFSClient writeChunk allocating new packet seqno=0, src=/megerlittlefile2/test08/part-m-00000, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
[2020-08-26 10:30:29:624] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 10:30:29:626] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #11
[2020-08-26 10:30:29:626] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 2ms
[2020-08-26 10:30:29:627] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 10:30:29:628] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-26 10:30:29:629] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]
[2020-08-26 10:30:29:637] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #12 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 10:30:29:639] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #12
[2020-08-26 10:30:29:639] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 2ms
[2020-08-26 10:30:29:640] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 10:30:29:640] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.74:50010
[2020-08-26 10:30:29:647] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 0
[2020-08-26 10:30:29:647] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:703)]Allocating new block
[2020-08-26 10:30:29:647] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 1
[2020-08-26 10:30:29:648] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:867)]Waiting for ack for: 1
[2020-08-26 10:30:29:660] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #13 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock
[2020-08-26 10:30:29:702] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #13
[2020-08-26 10:30:29:703] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: addBlock took 44ms
[2020-08-26 10:30:29:708] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1679)]pipeline = [DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]]
[2020-08-26 10:30:29:710] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:255)]Connecting to datanode 192.168.163.73:50010
[2020-08-26 10:30:29:715] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:267)]Send buf size 131768
[2020-08-26 10:30:29:715] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-26 10:30:29:798] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761845_21139 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 26445
[2020-08-26 10:30:29:965] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 12966257
[2020-08-26 10:30:29:972] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761845_21139 sending packet packet seqno: 1 offsetInBlock: 26445 lastPacketInBlock: true lastByteOffsetInBlock: 26445
[2020-08-26 10:30:29:998] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 23273943
[2020-08-26 10:30:29:998] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:622)]Closing old block BP-972878040-192.168.163.71-1586355896590:blk_1073761845_21139
[2020-08-26 10:30:30:002] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root sending #14 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete
[2020-08-26 10:30:30:020] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root got value #14
[2020-08-26 10:30:30:020] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: complete took 19ms
[2020-08-26 10:30:40:005] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 10:30:40:005] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1167836532) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 10:30:59:527] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-26 10:30:59:539] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@53fd5efb
[2020-08-26 10:30:59:540] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@53fd5efb
[2020-08-26 10:30:59:540] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@53fd5efb
[2020-08-26 10:30:59:540] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 10:30:59:540] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 10:55:10:913] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:107)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 10:55:10:925] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 10:55:10:926] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 10:55:10:974] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 10:55:10:982] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 10:55:10:983] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 10:55:10:984] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 10:55:10:985] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 10:55:10:986] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 10:55:11:045] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 10:55:11:049] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 10:55:11:052] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 10:55:11:054] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 10:55:11:054] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 10:55:11:058] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 10:55:11:059] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 10:55:11:060] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 10:55:11:143] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 10:55:11:150] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 10:55:11:151] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 10:55:11:158] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 10:55:11:159] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 10:55:11:159] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 10:55:11:159] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 10:55:11:209] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 10:55:11:216] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 10:55:11:453] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:55:11:453] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:55:11:453] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:55:11:453] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:55:11:454] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:55:11:454] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:55:11:453] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:55:11:455] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:55:11:455] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:55:11:454] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:55:11:454] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:55:11:455] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:55:11:468] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 10:55:11:468] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 10:55:11:468] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 10:55:11:487] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 10:55:11:488] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 10:55:11:487] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 10:55:11:488] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:55:11:488] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:55:11:488] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:55:11:489] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:55:11:488] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 10:55:11:489] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:55:11:489] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:55:11:490] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 10:55:11:491] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:55:11:491] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 10:55:11:491] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:55:11:492] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 10:55:11:505] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 10:55:11:505] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 10:55:11:505] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 10:55:11:523] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@1806aa49
[2020-08-26 10:55:11:542] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6a01fd3a
[2020-08-26 10:55:11:542] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6a01fd3a
[2020-08-26 10:55:11:542] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6a01fd3a
[2020-08-26 10:55:12:075] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 10:55:12:080] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 10:55:12:080] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 10:55:12:080] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 10:55:12:086] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6a01fd3a
[2020-08-26 10:55:12:087] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6a01fd3a
[2020-08-26 10:55:12:108] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 10:55:12:108] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 10:55:12:108] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 10:55:12:109] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 10:55:12:160] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 10:55:12:162] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 10:55:12:168] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 10:55:12:169] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:55:12:175] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #1
[2020-08-26 10:55:12:175] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #2
[2020-08-26 10:55:12:175] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 85ms
[2020-08-26 10:55:12:175] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 85ms
[2020-08-26 10:55:12:175] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 10:55:12:176] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 86ms
[2020-08-26 10:55:12:200] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:55:12:200] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:55:12:200] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:55:12:201] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #4
[2020-08-26 10:55:12:201] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 10:55:12:201] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #5
[2020-08-26 10:55:12:201] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 10:55:12:201] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #3
[2020-08-26 10:55:12:202] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 10:55:12:202] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:55:12:203] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #6
[2020-08-26 10:55:12:203] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 10:55:12:204] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 10:55:12:205] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #7
[2020-08-26 10:55:12:205] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 10:55:12:206] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1273)]/megerlittlefile2/test08/part-m-00000: masked=rw-r--r--
[2020-08-26 10:55:12:251] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
[2020-08-26 10:55:12:265] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #8
[2020-08-26 10:55:12:265] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: create took 14ms
[2020-08-26 10:55:12:278] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.computePacketChunkSize(DFSOutputStream.java:389)]computePacketChunkSize: src=/megerlittlefile2/test08/part-m-00000, chunkSize=516, chunksPerPacket=126, packetSize=65016
[2020-08-26 10:55:12:281] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)]Lease renewer daemon for [DFSClient_NONMAPREDUCE_-364725615_13] with renew id 1 started
[2020-08-26 10:55:12:292] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 10:55:12:297] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #9
[2020-08-26 10:55:12:298] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 6ms
[2020-08-26 10:55:12:316] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 10:55:12:320] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-26 10:55:12:336] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
[2020-08-26 10:55:12:338] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #10
[2020-08-26 10:55:12:339] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getServerDefaults took 3ms
[2020-08-26 10:55:12:352] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-26 10:55:12:473] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:418)]DFSClient writeChunk allocating new packet seqno=0, src=/megerlittlefile2/test08/part-m-00000, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
[2020-08-26 10:55:12:476] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 10:55:12:481] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #11
[2020-08-26 10:55:12:481] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 5ms
[2020-08-26 10:55:12:482] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 10:55:12:483] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-26 10:55:12:492] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #12 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 10:55:12:496] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #12
[2020-08-26 10:55:12:497] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 6ms
[2020-08-26 10:55:12:499] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 10:55:12:499] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.74:50010
[2020-08-26 10:55:12:500] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.74, datanodeId = DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-26 10:55:12:543] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 0
[2020-08-26 10:55:12:544] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:703)]Allocating new block
[2020-08-26 10:55:12:544] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 1
[2020-08-26 10:55:12:546] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:867)]Waiting for ack for: 1
[2020-08-26 10:55:12:565] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #13 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock
[2020-08-26 10:55:12:584] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #13
[2020-08-26 10:55:12:585] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: addBlock took 20ms
[2020-08-26 10:55:12:589] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1679)]pipeline = [DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]]
[2020-08-26 10:55:12:590] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:255)]Connecting to datanode 192.168.163.74:50010
[2020-08-26 10:55:12:594] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:267)]Send buf size 131768
[2020-08-26 10:55:12:595] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.74, datanodeId = DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]
[2020-08-26 10:55:12:660] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761846_21140 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 26445
[2020-08-26 10:55:12:805] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 9427073
[2020-08-26 10:55:12:811] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761846_21140 sending packet packet seqno: 1 offsetInBlock: 26445 lastPacketInBlock: true lastByteOffsetInBlock: 26445
[2020-08-26 10:55:12:827] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 4858763
[2020-08-26 10:55:12:828] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:622)]Closing old block BP-972878040-192.168.163.71-1586355896590:blk_1073761846_21140
[2020-08-26 10:55:12:833] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root sending #14 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete
[2020-08-26 10:55:12:839] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root got value #14
[2020-08-26 10:55:12:840] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: complete took 8ms
[2020-08-26 10:55:22:836] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 10:55:22:837] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1360933360) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 10:55:42:389] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-26 10:56:12:505] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-26 10:56:13:511] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:473)]Lease renewer daemon for [] with renew id 1 expired
[2020-08-26 10:56:13:511] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:318)]Lease renewer daemon for [] with renew id 1 exited
[2020-08-26 10:59:20:058] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6a01fd3a
[2020-08-26 10:59:20:060] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@6a01fd3a
[2020-08-26 10:59:20:061] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@6a01fd3a
[2020-08-26 10:59:20:061] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 10:59:20:062] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 11:06:31:341] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:107)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 11:06:31:356] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 11:06:31:357] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 11:06:31:420] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 11:06:31:428] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 11:06:31:429] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 11:06:31:429] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 11:06:31:430] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 11:06:31:431] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 11:06:31:542] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 11:06:31:548] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 11:06:31:553] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 11:06:31:554] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 11:06:31:554] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 11:06:31:560] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 11:06:31:561] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 11:06:31:563] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 11:06:31:689] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 11:06:31:705] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 11:06:31:707] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 11:06:31:710] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 11:06:31:710] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 11:06:31:710] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 11:06:31:713] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 11:06:31:804] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 11:06:31:814] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 11:06:32:129] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:06:32:129] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:06:32:129] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:06:32:129] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:06:32:129] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:06:32:130] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:06:32:129] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:06:32:130] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:06:32:130] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:06:32:130] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:06:32:131] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:06:32:130] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:06:32:139] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 11:06:32:139] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 11:06:32:139] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 11:06:32:153] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 11:06:32:153] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 11:06:32:153] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 11:06:32:154] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:06:32:153] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:06:32:154] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:06:32:154] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:06:32:154] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:06:32:154] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:06:32:154] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:06:32:154] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:06:32:155] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:06:32:155] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:06:32:155] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:06:32:155] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:06:32:166] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 11:06:32:166] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 11:06:32:166] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 11:06:32:180] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@3b22af27
[2020-08-26 11:06:32:189] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@b4cee42
[2020-08-26 11:06:32:190] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@b4cee42
[2020-08-26 11:06:32:190] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@b4cee42
[2020-08-26 11:06:32:552] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 11:06:32:557] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 11:06:32:557] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 11:06:32:557] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 11:06:32:564] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@b4cee42
[2020-08-26 11:06:32:564] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@b4cee42
[2020-08-26 11:06:32:584] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 11:06:32:584] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 11:06:32:584] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 11:06:32:585] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 11:06:32:625] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (26084579) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 11:06:32:627] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 11:06:32:630] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:06:32:631] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 11:06:32:635] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #1
[2020-08-26 11:06:32:636] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 68ms
[2020-08-26 11:06:32:636] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #2
[2020-08-26 11:06:32:636] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 68ms
[2020-08-26 11:06:32:636] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 11:06:32:637] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 69ms
[2020-08-26 11:06:32:658] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:06:32:658] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:06:32:659] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:06:32:659] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #4
[2020-08-26 11:06:32:659] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 11:06:32:659] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #5
[2020-08-26 11:06:32:659] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 11:06:32:660] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:06:32:664] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #3
[2020-08-26 11:06:32:664] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 6ms
[2020-08-26 11:06:32:664] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #6
[2020-08-26 11:06:32:664] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 4ms
[2020-08-26 11:06:32:665] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:06:32:668] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #7
[2020-08-26 11:06:32:668] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 4ms
[2020-08-26 11:06:32:669] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1273)]/megerlittlefile2/test08/part-m-00000: masked=rw-r--r--
[2020-08-26 11:06:32:705] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
[2020-08-26 11:06:32:720] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #8
[2020-08-26 11:06:32:720] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: create took 15ms
[2020-08-26 11:06:32:729] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.computePacketChunkSize(DFSOutputStream.java:389)]computePacketChunkSize: src=/megerlittlefile2/test08/part-m-00000, chunkSize=516, chunksPerPacket=126, packetSize=65016
[2020-08-26 11:06:32:731] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)]Lease renewer daemon for [DFSClient_NONMAPREDUCE_-312600292_12] with renew id 1 started
[2020-08-26 11:06:32:737] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 11:06:32:740] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #9
[2020-08-26 11:06:32:740] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 3ms
[2020-08-26 11:06:32:748] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 11:06:32:750] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.74:50010
[2020-08-26 11:06:32:757] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
[2020-08-26 11:06:32:758] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #10
[2020-08-26 11:06:32:758] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getServerDefaults took 1ms
[2020-08-26 11:06:32:764] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.74, datanodeId = DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-26 11:06:32:878] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:418)]DFSClient writeChunk allocating new packet seqno=0, src=/megerlittlefile2/test08/part-m-00000, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
[2020-08-26 11:06:32:884] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 11:06:32:888] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #11
[2020-08-26 11:06:32:888] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 4ms
[2020-08-26 11:06:32:889] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 11:06:32:889] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-26 11:06:32:891] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]
[2020-08-26 11:06:32:902] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #12 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 11:06:32:903] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #12
[2020-08-26 11:06:32:904] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 2ms
[2020-08-26 11:06:32:904] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.74:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 11:06:32:904] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.74:50010
[2020-08-26 11:06:32:909] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 0
[2020-08-26 11:06:32:909] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:703)]Allocating new block
[2020-08-26 11:06:32:909] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 1
[2020-08-26 11:06:32:909] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:867)]Waiting for ack for: 1
[2020-08-26 11:06:32:919] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #13 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock
[2020-08-26 11:06:32:929] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #13
[2020-08-26 11:06:32:929] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: addBlock took 10ms
[2020-08-26 11:06:32:932] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1679)]pipeline = [DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]
[2020-08-26 11:06:32:932] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:255)]Connecting to datanode 192.168.163.72:50010
[2020-08-26 11:06:32:935] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:267)]Send buf size 131768
[2020-08-26 11:06:32:935] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.72, datanodeId = DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]
[2020-08-26 11:06:32:960] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761865_21159 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 26445
[2020-08-26 11:06:33:098] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 4351073
[2020-08-26 11:06:33:100] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761865_21159 sending packet packet seqno: 1 offsetInBlock: 26445 lastPacketInBlock: true lastByteOffsetInBlock: 26445
[2020-08-26 11:06:33:110] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 6035093
[2020-08-26 11:06:33:110] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:622)]Closing old block BP-972878040-192.168.163.71-1586355896590:blk_1073761865_21159
[2020-08-26 11:06:33:112] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (26084579) connection to /192.168.163.71:8020 from root sending #14 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete
[2020-08-26 11:06:33:119] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (26084579) connection to /192.168.163.71:8020 from root got value #14
[2020-08-26 11:06:33:119] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: complete took 7ms
[2020-08-26 11:06:43:116] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (26084579) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 11:06:43:117] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (26084579) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 11:07:02:847] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-26 11:07:32:954] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-26 11:07:33:962] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:473)]Lease renewer daemon for [] with renew id 1 expired
[2020-08-26 11:07:33:965] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:318)]Lease renewer daemon for [] with renew id 1 exited
[2020-08-26 11:10:17:860] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@b4cee42
[2020-08-26 11:10:17:864] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@b4cee42
[2020-08-26 11:10:17:865] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@b4cee42
[2020-08-26 11:10:17:866] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 11:10:17:870] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 11:28:51:911] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:31)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:107)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 11:28:51:932] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 11:28:51:932] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 11:28:51:991] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 11:28:51:998] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 11:28:51:999] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 11:28:51:999] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 11:28:51:999] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 11:28:52:000] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 11:28:52:076] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 11:28:52:080] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 11:28:52:083] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 11:28:52:084] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 11:28:52:084] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 11:28:52:089] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 11:28:52:090] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 11:28:52:091] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 11:28:52:171] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 11:28:52:183] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 11:28:52:185] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 11:28:52:186] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 11:28:52:186] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 11:28:52:186] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 11:28:52:187] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 11:28:52:246] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 11:28:52:251] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 11:28:52:608] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:28:52:609] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:28:52:608] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:28:52:608] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:28:52:610] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:28:52:610] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:28:52:610] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:28:52:610] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:28:52:611] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:28:52:611] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:28:52:611] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:28:52:612] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:28:52:627] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 11:28:52:627] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 11:28:52:627] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 11:28:52:662] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 11:28:52:663] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 11:28:52:664] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 11:28:52:664] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:28:52:664] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:28:52:665] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:28:52:665] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 11:28:52:666] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:28:52:665] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:28:52:665] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 11:28:52:667] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:28:52:666] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:28:52:667] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 11:28:52:669] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:28:52:668] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 11:28:52:690] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 11:28:52:690] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 11:28:52:690] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 11:28:52:730] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@68f6c337
[2020-08-26 11:28:52:746] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@33fccd61
[2020-08-26 11:28:52:746] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@33fccd61
[2020-08-26 11:28:52:747] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@33fccd61
[2020-08-26 11:28:53:227] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 11:28:53:231] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 11:28:53:231] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 11:28:53:231] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 11:28:53:237] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@33fccd61
[2020-08-26 11:28:53:237] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@33fccd61
[2020-08-26 11:28:53:257] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 11:28:53:257] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 11:28:53:257] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 11:28:53:258] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 11:28:53:295] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 11:28:53:297] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 11:28:53:301] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:28:53:301] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 11:28:53:305] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #1
[2020-08-26 11:28:53:305] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #2
[2020-08-26 11:28:53:306] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 11:28:53:305] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 64ms
[2020-08-26 11:28:53:306] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 65ms
[2020-08-26 11:28:53:306] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 65ms
[2020-08-26 11:28:53:327] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:28:53:327] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:28:53:328] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:28:53:329] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #3
[2020-08-26 11:28:53:329] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 11:28:53:329] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #5
[2020-08-26 11:28:53:329] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 11:28:53:329] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #4
[2020-08-26 11:28:53:329] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 11:28:53:330] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:28:53:330] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #6
[2020-08-26 11:28:53:331] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 11:28:53:331] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 11:28:53:332] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #7
[2020-08-26 11:28:53:332] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 11:28:53:333] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1273)]/megerlittlefile2/test08/part-m-00000: masked=rw-r--r--
[2020-08-26 11:28:53:375] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
[2020-08-26 11:28:53:385] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #8
[2020-08-26 11:28:53:386] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: create took 11ms
[2020-08-26 11:28:53:395] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.computePacketChunkSize(DFSOutputStream.java:389)]computePacketChunkSize: src=/megerlittlefile2/test08/part-m-00000, chunkSize=516, chunksPerPacket=126, packetSize=65016
[2020-08-26 11:28:53:398] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)]Lease renewer daemon for [DFSClient_NONMAPREDUCE_1386332511_11] with renew id 1 started
[2020-08-26 11:28:53:403] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 11:28:53:405] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #9
[2020-08-26 11:28:53:405] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 2ms
[2020-08-26 11:28:53:413] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761088_20384; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 11:28:53:415] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-26 11:28:53:422] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
[2020-08-26 11:28:53:423] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #10
[2020-08-26 11:28:53:423] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getServerDefaults took 1ms
[2020-08-26 11:28:53:428] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.73, datanodeId = DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]
[2020-08-26 11:28:53:544] [DEBUG] [method:org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:418)]DFSClient writeChunk allocating new packet seqno=0, src=/megerlittlefile2/test08/part-m-00000, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0
[2020-08-26 11:28:53:549] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 11:28:53:554] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #11
[2020-08-26 11:28:53:554] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 5ms
[2020-08-26 11:28:53:556] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761089_20385; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK], DatanodeInfoWithStorage[192.168.163.72:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 11:28:53:556] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.72:50010
[2020-08-26 11:28:53:559] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.72, datanodeId = DatanodeInfoWithStorage[192.168.163.72:50010,DS-0d822d7a-3582-4203-9238-3332b10d71cc,DISK]
[2020-08-26 11:28:53:576] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #12 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[2020-08-26 11:28:53:580] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #12
[2020-08-26 11:28:53:580] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getBlockLocations took 4ms
[2020-08-26 11:28:53:580] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:327)]newInfo = LocatedBlocks{
  fileLength=8815
  underConstruction=false
  blocks=[LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}]
  lastLocatedBlock=LocatedBlock{BP-972878040-192.168.163.71-1586355896590:blk_1073761090_20386; getBlockSize()=8815; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK], DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]]}
  isLastBlockComplete=true}
[2020-08-26 11:28:53:581] [DEBUG] [method:org.apache.hadoop.hdfs.DFSInputStream.getBestNodeDNAddrPair(DFSInputStream.java:1125)]Connecting to datanode 192.168.163.73:50010
[2020-08-26 11:28:53:587] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 0
[2020-08-26 11:28:53:587] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:703)]Allocating new block
[2020-08-26 11:28:53:587] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.queuePacket(DataStreamer.java:1956)]Queued packet 1
[2020-08-26 11:28:53:588] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:867)]Waiting for ack for: 1
[2020-08-26 11:28:53:597] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #13 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock
[2020-08-26 11:28:53:603] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #13
[2020-08-26 11:28:53:603] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: addBlock took 7ms
[2020-08-26 11:28:53:605] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1679)]pipeline = [DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK], DatanodeInfoWithStorage[192.168.163.73:50010,DS-cfe402a9-0d6c-4963-8bc4-41ab80cf7670,DISK]]
[2020-08-26 11:28:53:605] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:255)]Connecting to datanode 192.168.163.74:50010
[2020-08-26 11:28:53:607] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:267)]Send buf size 131768
[2020-08-26 11:28:53:608] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:244)]SASL client skipping handshake in unsecured configuration for addr = /192.168.163.74, datanodeId = DatanodeInfoWithStorage[192.168.163.74:50010,DS-2eba532b-7a17-4c89-a554-9b744fee7117,DISK]
[2020-08-26 11:28:53:629] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761877_21171 sending packet packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 26445
[2020-08-26 11:28:53:756] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 1370867
[2020-08-26 11:28:53:759] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:760)]DataStreamer block BP-972878040-192.168.163.71-1586355896590:blk_1073761877_21171 sending packet packet seqno: 1 offsetInBlock: 26445 lastPacketInBlock: true lastByteOffsetInBlock: 26445
[2020-08-26 11:28:53:773] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1088)]DFSClient seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 4465450
[2020-08-26 11:28:53:774] [DEBUG] [method:org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:622)]Closing old block BP-972878040-192.168.163.71-1586355896590:blk_1073761877_21171
[2020-08-26 11:28:53:777] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root sending #14 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete
[2020-08-26 11:28:53:782] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root got value #14
[2020-08-26 11:28:53:782] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: complete took 6ms
[2020-08-26 11:29:03:783] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 11:29:03:784] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1991029159) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 11:29:23:513] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-26 11:29:53:639] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:443)]Lease renewer daemon for [] with renew id 1 executed
[2020-08-26 11:29:54:648] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:473)]Lease renewer daemon for [] with renew id 1 expired
[2020-08-26 11:29:54:653] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:318)]Lease renewer daemon for [] with renew id 1 exited
[2020-08-26 13:23:41:054] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@33fccd61
[2020-08-26 13:23:41:066] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@33fccd61
[2020-08-26 13:23:41:067] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@33fccd61
[2020-08-26 13:23:41:068] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 13:23:41:075] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 13:29:49:559] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:29:49:570] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 13:29:49:570] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 13:29:49:645] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 13:29:49:653] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 13:29:49:653] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 13:29:49:654] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 13:29:49:654] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 13:29:49:656] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 13:29:49:745] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 13:29:49:752] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 13:29:49:757] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 13:29:49:758] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 13:29:49:758] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 13:29:49:759] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 13:29:49:760] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 13:29:49:763] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 13:29:49:875] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 13:29:49:889] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 13:29:49:890] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 13:29:49:892] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 13:29:49:892] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 13:29:49:893] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 13:29:49:895] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 13:29:49:958] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 13:29:49:982] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 13:29:50:273] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:29:50:273] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:29:50:273] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:29:50:274] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:29:50:284] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 13:29:50:297] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 13:29:50:298] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:29:50:298] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:29:50:298] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:29:50:298] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:29:50:308] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:29:50:322] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@669a97b5
[2020-08-26 13:29:50:333] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@7da0b4a7
[2020-08-26 13:29:58:268] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:29:58:284] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 13:29:58:284] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 13:29:58:344] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 13:29:58:351] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 13:29:58:351] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-26 13:29:58:352] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-26 13:29:58:353] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-26 13:29:58:354] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 13:29:58:428] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 13:29:58:432] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 13:29:58:434] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 13:29:58:435] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 13:29:58:435] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 13:29:58:437] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 13:29:58:438] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 13:29:58:441] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 13:29:58:569] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 13:29:58:585] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 13:29:58:588] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 13:29:58:591] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 13:29:58:593] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 13:29:58:594] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 13:29:58:595] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 13:29:58:672] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 13:29:58:696] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 13:30:03:552] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:30:03:563] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 13:30:03:563] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 13:30:03:605] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 13:30:03:612] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 13:30:03:612] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-26 13:30:03:613] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-26 13:30:03:614] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-26 13:30:03:616] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 13:30:03:691] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 13:30:03:697] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 13:30:03:700] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 13:30:03:701] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 13:30:03:702] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 13:30:03:704] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 13:30:03:705] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 13:30:03:708] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 13:30:03:780] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 13:30:03:788] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 13:30:03:790] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 13:30:03:791] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 13:30:03:791] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 13:30:03:791] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 13:30:03:792] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 13:30:03:863] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 13:30:03:892] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 13:30:04:114] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:30:04:114] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:30:04:115] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:30:04:115] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:30:04:124] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 13:30:04:140] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 13:30:04:140] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:30:04:141] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:30:04:141] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:30:04:141] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:30:04:152] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:30:04:170] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@70e084e2
[2020-08-26 13:30:04:189] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:30:04:570] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 13:30:04:576] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 13:30:04:610] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:30:04:611] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 13:30:04:665] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 13:30:04:697] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:30:04:731] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 13:30:04:736] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:30:04:737] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:30:04:737] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:30:04:738] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:30:04:738] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 13:30:04:739] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 13:30:04:739] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:30:04:752] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 13:30:04:752] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 14ms
[2020-08-26 13:30:04:781] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 13:30:04:797] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 13:30:04:797] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 17ms
[2020-08-26 13:30:14:672] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 13:30:14:672] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-26 13:30:14:785] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 13:30:14:786] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 13:30:14:925] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:30:14:925] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:30:14:925] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:30:14:925] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:30:14:926] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 13:30:14:926] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 13:31:06:583] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:31:06:596] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 13:31:06:596] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 13:31:06:651] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 13:31:06:658] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 13:31:06:658] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 13:31:06:659] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 13:31:06:660] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 13:31:06:661] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 13:31:06:725] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 13:31:06:729] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 13:31:06:732] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 13:31:06:733] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 13:31:06:733] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 13:31:06:735] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 13:31:06:735] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 13:31:06:738] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 13:31:06:819] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 13:31:06:827] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 13:31:06:828] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 13:31:06:828] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 13:31:06:829] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 13:31:06:829] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 13:31:06:829] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 13:31:06:877] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 13:31:06:899] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 13:31:07:117] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:31:07:117] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:31:07:117] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:31:07:117] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:31:07:128] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 13:31:07:151] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 13:31:07:151] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:31:07:152] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:31:07:152] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:31:07:152] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:31:07:166] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:31:07:183] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@70e084e2
[2020-08-26 13:31:07:198] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:31:07:541] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 13:31:07:546] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 13:31:07:573] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:31:07:573] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 13:31:07:616] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 13:31:07:636] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:31:07:644] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 13:31:07:648] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:31:07:649] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:31:07:649] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:31:07:649] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:31:07:649] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 13:31:07:650] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 13:31:07:650] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:31:07:651] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 13:31:07:652] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 13:31:07:673] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 13:31:07:675] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 13:31:07:675] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 13:31:17:624] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 13:31:17:625] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1981025978) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-26 13:31:17:677] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 13:31:17:677] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1981025978) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 13:37:06:316] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:37:06:320] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:37:06:320] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:37:06:321] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@4fb47c28
[2020-08-26 13:37:06:322] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 13:37:06:324] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 13:43:15:669] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:43:15:692] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 13:43:15:692] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 13:43:15:774] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 13:43:15:784] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 13:43:15:785] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 13:43:15:786] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 13:43:15:786] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 13:43:15:789] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 13:43:15:870] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 13:43:15:877] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 13:43:15:882] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 13:43:15:884] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 13:43:15:884] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 13:43:15:885] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 13:43:15:886] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 13:43:15:889] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 13:43:15:979] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 13:43:15:991] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 13:43:15:993] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 13:43:15:995] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 13:43:15:995] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 13:43:15:995] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 13:43:15:996] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 13:43:16:063] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 13:43:16:092] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 13:43:16:349] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:43:16:349] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:43:16:350] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:43:16:350] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:43:16:361] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 13:43:16:378] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 13:43:16:378] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:43:16:378] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:43:16:379] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:43:16:379] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:43:16:393] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:43:16:410] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@46672bcf
[2020-08-26 13:43:16:422] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@12781d7a
[2020-08-26 13:43:16:783] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 13:43:16:788] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 13:43:16:814] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:43:16:814] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 13:43:16:860] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1031835834) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 13:43:16:886] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1031835834) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:43:16:900] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1031835834) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 13:43:16:904] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:43:16:905] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:43:16:905] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@12781d7a
[2020-08-26 13:43:16:906] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:43:16:906] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 13:43:16:907] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1031835834) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 13:43:16:907] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1031835834) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:43:16:909] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1031835834) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 13:43:16:909] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 13:43:16:931] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1031835834) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 13:43:16:932] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1031835834) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 13:43:16:932] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 13:43:26:868] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1031835834) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 13:43:26:868] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1031835834) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-26 13:43:26:937] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1031835834) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 13:43:26:937] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1031835834) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 13:47:15:792] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@12781d7a
[2020-08-26 13:47:15:799] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@12781d7a
[2020-08-26 13:47:15:799] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@12781d7a
[2020-08-26 13:47:15:799] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@12781d7a
[2020-08-26 13:47:15:799] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 13:47:15:801] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 13:47:49:225] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:47:49:241] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 13:47:49:242] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 13:47:49:282] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 13:47:49:289] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 13:47:49:289] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 13:47:49:289] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 13:47:49:290] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 13:47:49:291] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 13:47:49:371] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 13:47:49:376] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 13:47:49:380] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 13:47:49:382] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 13:47:49:382] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 13:47:49:388] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 13:47:49:389] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 13:47:49:390] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 13:47:49:471] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 13:47:49:481] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 13:47:49:482] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 13:47:49:484] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 13:47:49:484] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 13:47:49:484] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 13:47:49:485] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 13:47:49:539] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 13:47:49:545] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 13:47:49:767] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:47:49:767] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:47:49:768] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:47:49:768] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:47:49:779] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 13:47:49:795] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 13:47:49:796] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:47:49:796] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:47:49:796] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:47:49:796] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:47:49:807] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:47:49:822] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@7c9f155d
[2020-08-26 13:47:49:833] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:47:50:159] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 13:47:50:164] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 13:47:50:192] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:47:50:193] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 13:47:50:238] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 13:47:50:240] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:47:50:254] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 13:47:50:259] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:47:50:260] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:47:50:260] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:47:50:260] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:47:50:260] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 13:47:50:261] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 13:47:50:262] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:47:50:263] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 13:47:50:263] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 13:47:50:285] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 13:47:50:287] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 13:47:50:287] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 13:48:00:244] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 13:48:00:246] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-26 13:48:00:290] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 13:48:00:291] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 13:51:54:897] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:51:54:902] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:51:54:903] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:51:54:903] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:51:54:903] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 13:51:54:907] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 13:52:47:893] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:52:47:912] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 13:52:47:912] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 13:52:47:986] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 13:52:48:000] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 13:52:48:002] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 13:52:48:003] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 13:52:48:006] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 13:52:48:009] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 13:52:48:113] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 13:52:48:117] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 13:52:48:120] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 13:52:48:121] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 13:52:48:121] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 13:52:48:126] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 13:52:48:128] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 13:52:48:129] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 13:52:48:212] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 13:52:48:228] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 13:52:48:230] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 13:52:48:232] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 13:52:48:233] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 13:52:48:234] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 13:52:48:235] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 13:52:48:334] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 13:52:48:339] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 13:52:48:603] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:52:48:604] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:52:48:604] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:52:48:605] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:52:48:614] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 13:52:48:631] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 13:52:48:631] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:52:48:632] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:52:48:632] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:52:48:632] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:52:48:648] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:52:48:668] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@7c9f155d
[2020-08-26 13:52:48:681] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:52:49:025] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 13:52:49:029] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 13:52:49:054] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:52:49:055] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 13:52:49:096] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 13:52:49:098] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:52:49:110] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 13:52:49:114] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:52:49:115] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:52:49:115] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:52:49:115] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:52:49:116] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 13:52:49:116] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 13:52:49:116] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:52:49:118] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 13:52:49:118] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 13:52:49:141] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 13:52:49:149] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 13:52:49:149] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 8ms
[2020-08-26 13:52:53:173] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:52:53:174] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:52:53:174] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:52:53:174] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 13:52:53:174] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 13:52:53:179] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 13:52:53:179] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 13:52:53:181] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 13:52:53:181] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 13:52:53:278] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 13:53:14:378] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:53:14:419] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 13:53:14:420] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 13:53:14:509] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 13:53:14:516] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 13:53:14:517] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 13:53:14:518] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 13:53:14:518] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 13:53:14:520] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 13:53:14:626] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 13:53:14:672] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 13:53:14:680] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 13:53:14:682] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 13:53:14:691] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 13:53:14:695] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 13:53:14:698] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 13:53:14:703] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 13:53:14:887] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 13:53:14:913] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 13:53:14:916] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 13:53:14:920] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 13:53:14:921] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 13:53:14:921] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 13:53:14:923] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 13:53:15:015] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 13:53:15:021] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 13:53:15:495] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:53:15:496] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:53:15:496] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:53:15:496] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:53:15:514] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 13:53:15:538] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 13:53:15:539] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 13:53:15:539] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 13:53:15:540] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 13:53:15:540] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 13:53:15:561] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:53:15:601] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@68e498ee
[2020-08-26 13:53:15:631] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@44644f95
[2020-08-26 13:53:16:551] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 13:53:16:558] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 13:53:16:611] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:53:16:613] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 13:53:16:679] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1921418412) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 13:53:16:681] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1921418412) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:53:16:696] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1921418412) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 13:53:16:706] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 13:53:16:709] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 13:53:16:710] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@44644f95
[2020-08-26 13:53:16:711] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 13:53:16:711] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 13:53:16:712] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1921418412) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 13:53:16:712] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1921418412) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 13:53:16:713] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1921418412) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 13:53:16:714] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 13:53:16:750] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1921418412) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 13:53:16:752] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1921418412) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 13:53:16:752] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 13:53:26:685] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1921418412) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 13:53:26:685] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1921418412) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-26 13:53:26:754] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1921418412) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 13:53:26:755] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1921418412) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 13:58:28:894] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@44644f95
[2020-08-26 13:58:28:900] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@44644f95
[2020-08-26 13:58:28:900] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@44644f95
[2020-08-26 13:58:28:900] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@44644f95
[2020-08-26 13:58:28:901] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 13:58:28:903] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:01:37:467] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:01:37:484] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:01:37:484] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:01:37:529] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 14:01:37:538] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 14:01:37:538] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 14:01:37:539] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 14:01:37:540] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 14:01:37:541] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:01:37:634] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:01:37:640] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:01:37:642] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:01:37:644] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:01:37:650] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:01:37:651] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:01:37:652] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:01:37:654] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:01:37:787] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:01:37:801] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:01:37:802] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:01:37:805] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:01:37:805] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:01:37:806] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:01:37:807] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:01:37:932] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:01:37:938] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:01:38:199] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:01:38:199] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:01:38:199] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:01:38:200] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:01:38:207] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:01:38:218] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:01:38:219] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:01:38:219] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:01:38:219] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:01:38:219] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:01:38:230] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:01:38:245] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@7c9f155d
[2020-08-26 14:01:38:258] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 14:01:39:029] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:01:39:041] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:01:39:073] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:01:39:074] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:01:39:128] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:01:39:130] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:01:39:143] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:01:39:147] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:01:39:148] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:01:39:148] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 14:01:39:149] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:01:39:149] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:01:39:150] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:01:39:150] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:01:39:151] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:01:39:152] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 14:01:39:173] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:01:39:177] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:01:39:178] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 5ms
[2020-08-26 14:01:49:134] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:01:49:135] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2108055355) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-26 14:01:49:177] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:01:49:177] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2108055355) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:03:23:036] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 14:03:23:040] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 14:03:23:040] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 14:03:23:040] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@55328f95
[2020-08-26 14:03:23:040] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:03:23:042] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:05:24:610] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:05:24:627] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:05:24:628] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:05:24:698] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:05:24:705] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:05:24:707] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-26 14:05:24:708] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-26 14:05:24:709] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-26 14:05:24:710] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:05:24:804] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:05:24:810] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:05:24:814] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:05:24:818] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:05:24:818] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:05:24:818] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:05:24:821] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:05:24:823] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:05:24:921] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:05:24:934] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:05:24:935] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:05:24:936] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:05:24:937] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:05:24:937] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:05:24:937] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:05:25:012] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:05:25:037] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:05:25:283] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:05:25:284] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:05:25:284] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:05:25:284] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:05:25:294] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:05:25:311] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:05:25:311] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:05:25:311] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:05:25:312] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:05:25:312] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:05:25:324] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:05:25:340] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@66485b12
[2020-08-26 14:05:25:352] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@2c050a61
[2020-08-26 14:05:25:701] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:05:25:705] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:06:01:791] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:06:01:795] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:06:01:865] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (834413685) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:06:01:948] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (834413685) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:06:01:960] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (834413685) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:06:01:986] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:06:01:995] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:06:01:997] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@2c050a61
[2020-08-26 14:06:01:998] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:06:02:000] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:06:02:002] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (834413685) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:06:02:002] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (834413685) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:06:02:004] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (834413685) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:06:02:004] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 6ms
[2020-08-26 14:06:02:100] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (834413685) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:06:02:103] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (834413685) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:06:02:103] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-26 14:06:38:866] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (834413685) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:06:38:867] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (834413685) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:06:38:868] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (834413685) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:06:38:868] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (834413685) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 14:08:03:443] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@2c050a61
[2020-08-26 14:08:03:444] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@2c050a61
[2020-08-26 14:08:03:445] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@2c050a61
[2020-08-26 14:08:03:445] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@2c050a61
[2020-08-26 14:08:03:445] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:08:03:447] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:11:41:260] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:11:41:274] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:11:41:274] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:11:41:312] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 14:11:41:317] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 14:11:41:317] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 14:11:41:317] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 14:11:41:318] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 14:11:41:319] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:11:41:390] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:11:41:393] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:11:41:395] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:11:41:397] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:11:41:399] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:11:41:403] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:11:41:404] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:11:41:405] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:11:41:471] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:11:41:479] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:11:41:480] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:11:41:481] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:11:41:481] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:11:41:482] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:11:41:482] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:11:41:519] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:11:41:523] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:11:41:764] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:11:41:765] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:11:41:765] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:11:41:766] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:11:41:775] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:11:41:796] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:11:41:797] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:11:41:798] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:11:41:798] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:11:41:798] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:11:41:808] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:11:41:826] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@4234bb63
[2020-08-26 14:11:41:843] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-26 14:11:42:292] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:11:42:296] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:11:42:324] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:11:42:326] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:11:42:390] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:11:42:392] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:11:42:402] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:11:42:406] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:11:42:408] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:11:42:408] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-26 14:11:42:409] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:11:42:409] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:11:42:410] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:11:42:410] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:11:42:412] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:11:42:413] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 4ms
[2020-08-26 14:11:42:436] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:11:42:438] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:11:42:438] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 14:11:52:397] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:11:52:398] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1986225467) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-26 14:11:52:441] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:11:52:441] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1986225467) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:12:04:700] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-26 14:12:04:700] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-26 14:12:04:700] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-26 14:12:04:701] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@6c71758e
[2020-08-26 14:12:04:701] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:12:04:701] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:18:14:046] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:18:14:059] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:18:14:059] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:18:14:114] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 14:18:14:121] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 14:18:14:122] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 14:18:14:122] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 14:18:14:123] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 14:18:14:124] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:18:14:193] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:18:14:200] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:18:14:204] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:18:14:205] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:18:14:205] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:18:14:207] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:18:14:208] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:18:14:211] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:18:14:300] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:18:14:309] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:18:14:310] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:18:14:311] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:18:14:311] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:18:14:312] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:18:14:312] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:18:14:363] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:18:14:387] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:18:14:596] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:18:14:597] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:18:14:597] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:18:14:597] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:18:14:606] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:18:14:622] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:18:14:622] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:18:14:622] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:18:14:623] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:18:14:623] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:18:14:634] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:18:14:649] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@6b59e331
[2020-08-26 14:18:14:660] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@4b96b5c6
[2020-08-26 14:18:14:990] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:18:14:995] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:18:15:026] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:18:15:027] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:18:15:076] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1033246872) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:18:15:102] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1033246872) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:18:15:111] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1033246872) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:18:15:115] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:18:15:116] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:18:15:117] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@4b96b5c6
[2020-08-26 14:18:15:117] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:18:15:117] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:18:15:118] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1033246872) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:18:15:118] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1033246872) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:18:15:120] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1033246872) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:18:15:120] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 14:18:15:146] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1033246872) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:18:15:147] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1033246872) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:18:15:148] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 14:18:25:084] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1033246872) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:18:25:084] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1033246872) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-26 14:18:25:149] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1033246872) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:18:25:149] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1033246872) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:18:47:415] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@4b96b5c6
[2020-08-26 14:18:47:415] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@4b96b5c6
[2020-08-26 14:18:47:415] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@4b96b5c6
[2020-08-26 14:18:47:416] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@4b96b5c6
[2020-08-26 14:18:47:416] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:18:47:416] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:19:59:394] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:19:59:404] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:19:59:404] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:19:59:443] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:19:59:447] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:19:59:448] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-26 14:19:59:448] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-26 14:19:59:448] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-26 14:19:59:449] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:19:59:508] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:19:59:512] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:19:59:514] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:19:59:515] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:19:59:516] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:19:59:518] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:19:59:519] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:19:59:524] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:19:59:652] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:19:59:666] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:19:59:667] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:19:59:669] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:19:59:669] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:19:59:669] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:19:59:670] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:19:59:718] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:19:59:741] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:19:59:981] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:19:59:981] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:19:59:981] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:19:59:982] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:19:59:991] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:20:00:004] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:20:00:005] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:20:00:005] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:20:00:005] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:20:00:005] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:20:00:015] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:20:00:032] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@923f7b
[2020-08-26 14:20:00:045] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@108f4c78
[2020-08-26 14:20:00:388] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:20:00:392] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:20:20:488] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:20:20:496] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:20:20:584] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1500708658) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:20:20:705] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1500708658) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:20:20:720] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1500708658) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:20:20:747] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:20:20:755] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:20:20:757] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@108f4c78
[2020-08-26 14:20:20:759] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:20:20:760] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:20:20:762] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1500708658) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:20:20:762] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1500708658) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:20:20:764] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1500708658) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:20:20:764] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 6ms
[2020-08-26 14:20:20:841] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1500708658) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:20:20:843] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1500708658) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:20:20:843] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-26 14:20:34:750] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1500708658) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:20:35:622] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1500708658) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:20:35:623] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1500708658) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 14:20:36:073] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1500708658) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:22:23:712] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@108f4c78
[2020-08-26 14:22:23:713] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@108f4c78
[2020-08-26 14:22:23:713] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@108f4c78
[2020-08-26 14:22:23:714] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@108f4c78
[2020-08-26 14:22:23:714] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:22:23:715] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:22:31:112] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:22:31:124] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:22:31:124] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:22:31:162] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 14:22:31:166] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 14:22:31:167] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 14:22:31:167] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 14:22:31:167] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 14:22:31:168] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:22:31:224] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:22:31:227] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:22:31:229] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:22:31:231] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:22:31:231] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:22:31:235] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:22:31:236] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:22:31:237] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:22:31:315] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:22:31:324] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:22:31:325] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:22:31:326] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:22:31:327] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:22:31:327] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:22:31:328] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:22:31:369] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:22:31:374] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:22:31:600] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:22:31:600] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:22:31:600] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:22:31:600] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:22:31:608] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:22:31:619] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:22:31:620] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:22:31:620] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:22:31:620] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:22:31:620] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:22:31:629] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:22:31:647] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@3709a009
[2020-08-26 14:22:31:659] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@32f872df
[2020-08-26 14:22:31:917] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:22:31:921] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:22:31:949] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:22:31:950] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:22:31:994] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (241851721) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:22:31:995] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (241851721) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:22:32:004] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (241851721) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:22:32:009] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:22:32:010] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:22:32:011] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@32f872df
[2020-08-26 14:22:32:012] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:22:32:012] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:22:32:013] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (241851721) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:22:32:013] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (241851721) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:22:32:015] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (241851721) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:22:32:015] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 14:22:32:036] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (241851721) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:22:32:038] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (241851721) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:22:32:038] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 14:22:40:051] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@32f872df
[2020-08-26 14:22:40:051] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@32f872df
[2020-08-26 14:22:40:051] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@32f872df
[2020-08-26 14:22:40:051] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@32f872df
[2020-08-26 14:22:40:051] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:22:40:057] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (241851721) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:22:40:057] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (241851721) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:22:40:058] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (241851721) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:22:40:058] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (241851721) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 14:22:40:156] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:24:27:789] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:24:27:807] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:24:27:807] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:24:27:866] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:24:27:872] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:24:27:873] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-26 14:24:27:873] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-26 14:24:27:873] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-26 14:24:27:874] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:24:27:941] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:24:27:945] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:24:27:948] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:24:27:949] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:24:27:949] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:24:27:953] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:24:27:955] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:24:27:956] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:24:28:036] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:24:28:043] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:24:28:044] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:24:28:045] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:24:28:046] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:24:28:046] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:24:28:047] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:24:28:105] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:24:28:109] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:24:28:357] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:24:28:358] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:24:28:358] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:24:28:358] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:24:28:369] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:24:28:386] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:24:28:386] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:24:28:386] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:24:28:386] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:24:28:387] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:24:28:400] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:24:28:416] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@2709cf35
[2020-08-26 14:24:28:426] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:24:28:720] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:24:28:725] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:24:28:751] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:24:28:752] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:24:28:794] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:24:28:796] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:24:28:806] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:24:28:810] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:24:28:810] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:24:28:811] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:24:28:811] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:24:28:811] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:24:28:812] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:24:28:812] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:24:28:814] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:24:28:814] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 14:24:28:835] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:24:28:836] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:24:28:836] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 14:24:35:741] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:24:35:741] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:24:35:741] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:24:35:741] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:24:35:741] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:24:35:746] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:24:35:746] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:24:35:747] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:24:35:747] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 14:24:35:846] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:26:32:655] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:26:32:669] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:26:32:669] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:26:36:929] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:26:36:940] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:26:36:940] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:26:36:983] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 14:26:36:989] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 14:26:36:990] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 14:26:36:990] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 14:26:36:991] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 14:26:36:992] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:26:37:091] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:26:37:098] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:26:37:104] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:26:37:106] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:26:37:106] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:26:37:109] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:26:37:111] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:26:37:119] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:26:37:237] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:26:37:247] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:26:37:248] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:26:37:249] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:26:37:250] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:26:37:250] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:26:37:251] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:26:37:302] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:26:37:332] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:26:37:579] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:26:37:579] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:26:37:579] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:26:37:579] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:26:37:588] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:26:37:603] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:26:37:603] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:26:37:603] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:26:37:604] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:26:37:604] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:26:37:615] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:26:37:632] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@76cb6b44
[2020-08-26 14:26:37:647] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@5c354a40
[2020-08-26 14:26:38:094] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:26:38:101] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:26:38:139] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:26:38:140] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:26:38:194] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1667271299) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:26:38:220] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1667271299) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:26:38:229] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1667271299) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:26:38:234] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:26:38:235] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:26:38:236] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@5c354a40
[2020-08-26 14:26:38:236] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:26:38:236] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:26:38:237] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (1667271299) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:26:38:237] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1667271299) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:26:38:239] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1667271299) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:26:38:240] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 4ms
[2020-08-26 14:26:38:270] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (1667271299) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:26:38:272] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (1667271299) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:26:38:273] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 3ms
[2020-08-26 14:27:03:602] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1667271299) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:27:03:603] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (1667271299) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:27:03:603] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1667271299) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 14:27:04:180] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (1667271299) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:28:15:683] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@5c354a40
[2020-08-26 14:28:15:684] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@5c354a40
[2020-08-26 14:28:15:685] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@5c354a40
[2020-08-26 14:28:15:685] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@5c354a40
[2020-08-26 14:28:15:685] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:28:15:686] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:29:24:366] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:29:24:378] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:29:24:378] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:29:24:442] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:29:24:452] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:29:24:453] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-26 14:29:24:453] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-26 14:29:24:454] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-26 14:29:24:456] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:29:24:540] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:29:24:544] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:29:24:547] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:29:24:547] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:29:24:548] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:29:24:551] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:29:24:552] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:29:24:553] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:29:24:626] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:29:24:634] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:29:24:635] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:29:24:636] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:29:24:636] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:29:24:636] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:29:24:636] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:29:24:710] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:29:24:718] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:29:24:943] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:29:24:943] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:29:24:944] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:29:24:944] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:29:24:953] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:29:24:966] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:29:24:967] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:29:24:967] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:29:24:967] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:29:24:967] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:29:24:986] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:29:25:005] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@11c6f655
[2020-08-26 14:29:25:016] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@21a73bc7
[2020-08-26 14:29:25:336] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:29:25:341] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:29:25:364] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:29:25:365] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:29:25:404] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (251046252) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:29:25:405] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (251046252) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:29:25:412] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (251046252) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:29:25:415] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:29:25:416] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:29:25:416] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@21a73bc7
[2020-08-26 14:29:25:417] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:29:25:417] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:29:25:418] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (251046252) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:29:25:418] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (251046252) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:29:25:420] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (251046252) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:29:25:420] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 3ms
[2020-08-26 14:29:25:439] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (251046252) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:29:25:440] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (251046252) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:29:25:441] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 14:29:32:753] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@21a73bc7
[2020-08-26 14:29:32:753] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@21a73bc7
[2020-08-26 14:29:32:754] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@21a73bc7
[2020-08-26 14:29:32:754] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@21a73bc7
[2020-08-26 14:29:32:754] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:29:32:758] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (251046252) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:29:32:758] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (251046252) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 14:29:32:759] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (251046252) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:29:32:760] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (251046252) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:29:32:858] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:29:48:356] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:29:48:366] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:29:48:366] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:29:48:398] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:29:48:402] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[2020-08-26 14:29:48:402] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
[2020-08-26 14:29:48:402] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[2020-08-26 14:29:48:403] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[2020-08-26 14:29:48:405] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:29:48:448] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:29:48:452] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:29:48:454] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:29:48:454] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:29:48:455] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:29:48:457] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:29:48:457] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:29:48:458] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:29:48:529] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:29:48:538] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:29:48:540] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:29:48:541] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:29:48:541] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:29:48:542] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:29:48:542] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:29:48:587] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:29:48:590] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:29:48:789] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:29:48:790] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:29:48:790] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:29:48:790] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:29:48:797] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:29:48:808] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:29:48:809] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:29:48:809] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:29:48:809] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:29:48:809] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:29:48:818] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:29:48:832] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@2709cf35
[2020-08-26 14:29:48:842] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:29:49:105] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:29:49:110] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:29:49:136] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:29:49:136] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:29:49:177] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:29:49:178] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:29:49:186] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:29:49:189] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:29:49:190] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:29:49:190] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:29:49:191] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:29:49:191] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:29:49:192] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:29:49:192] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:29:49:193] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:29:49:193] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 2ms
[2020-08-26 14:29:49:210] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:29:49:211] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:29:49:211] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 1ms
[2020-08-26 14:29:50:125] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:29:50:126] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:29:50:126] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:29:50:126] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@666719a8
[2020-08-26 14:29:50:126] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 14:29:50:130] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:29:50:130] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2063742405) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 14:29:50:131] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:29:50:131] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (2063742405) connection to /192.168.163.71:8020 from root: stopped, remaining connections 0
[2020-08-26 14:29:50:230] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
[2020-08-26 14:59:40:230] [DEBUG] [method:org.apache.hadoop.util.Shell.<clinit>(Shell.java:500)]Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.ebchinatech.util.MergeUtil.getFilSystem(MergeUtil.java:29)
	at com.ebchinatech.util.MergeFilePath.<init>(MergeFilePath.java:42)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:59:40:242] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:779)]setsid is not available on this machine. So not using it.
[2020-08-26 14:59:40:243] [DEBUG] [method:org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:793)]setsid exited with exit code 0
[2020-08-26 14:59:40:291] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])
[2020-08-26 14:59:40:298] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])
[2020-08-26 14:59:40:298] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])
[2020-08-26 14:59:40:299] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since startup])
[2020-08-26 14:59:40:300] [DEBUG] [method:org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42)]field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, about=, always=false, type=DEFAULT, valueName=Time, value=[Renewal failures since last successful login])
[2020-08-26 14:59:40:303] [DEBUG] [method:org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:231)]UgiMetrics, User and group related metrics
[2020-08-26 14:59:40:377] [DEBUG] [method:org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:89)]Kerberos krb5 configuration not found, setting default realm to empty
[2020-08-26 14:59:40:380] [DEBUG] [method:org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:448)] Creating new Groups object
[2020-08-26 14:59:40:383] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46)]Trying to load the custom-built native-hadoop library...
[2020-08-26 14:59:40:384] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55)]Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[2020-08-26 14:59:40:385] [DEBUG] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56)]java.library.path=/Users/fangnailiang/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[2020-08-26 14:59:40:388] [WARN ] [method:org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2020-08-26 14:59:40:389] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:41)]Falling back to shell based
[2020-08-26 14:59:40:390] [DEBUG] [method:org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:45)]Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[2020-08-26 14:59:40:464] [DEBUG] [method:org.apache.hadoop.security.Groups.<init>(Groups.java:152)]Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[2020-08-26 14:59:40:472] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:240)]hadoop login
[2020-08-26 14:59:40:473] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:175)]hadoop login commit
[2020-08-26 14:59:40:474] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:211)]Using user: "root" with name root
[2020-08-26 14:59:40:474] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:221)]User entry: "root"
[2020-08-26 14:59:40:474] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)]Assuming keytab is managed externally since logged in from subject.
[2020-08-26 14:59:40:474] [DEBUG] [method:org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:881)]UGI loginUser:root (auth:SIMPLE)
[2020-08-26 14:59:40:534] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSamplers(Tracer.java:106)]sampler.classes = ; loaded no samplers
[2020-08-26 14:59:40:545] [DEBUG] [method:org.apache.htrace.core.Tracer$Builder.loadSpanReceivers(Tracer.java:128)]span.receiver.classes = ; loaded no span receivers
[2020-08-26 14:59:40:803] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:59:40:804] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:59:40:803] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:59:40:804] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:59:40:804] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:59:40:804] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:59:40:804] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:59:40:805] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:59:40:813] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:59:40:813] [DEBUG] [method:org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:321)]Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[2020-08-26 14:59:40:827] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:59:40:828] [DEBUG] [method:org.apache.hadoop.hdfs.HAUtilClient.cloneDelegationTokenForLogicalUri(HAUtilClient.java:145)]No HA service delegation token found for logical URI hdfs://mycluster
[2020-08-26 14:59:40:828] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:59:40:828] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:59:40:828] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:59:40:829] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:59:40:828] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:646)]dfs.client.use.legacy.blockreader.local = false
[2020-08-26 14:59:40:829] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:648)]dfs.client.read.shortcircuit = false
[2020-08-26 14:59:40:829] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:650)]dfs.client.domain.socket.data.traffic = false
[2020-08-26 14:59:40:829] [DEBUG] [method:org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf.<init>(DfsClientConf.java:652)]dfs.domain.socket.path = 
[2020-08-26 14:59:40:841] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:59:40:841] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:59:40:856] [DEBUG] [method:org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:275)]rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@75261e97
[2020-08-26 14:59:40:866] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@77c799e8
[2020-08-26 14:59:40:866] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@77c799e8
[2020-08-26 14:59:41:197] [DEBUG] [method:org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:111)]Both short-circuit local reads and UNIX domain socket are disabled.
[2020-08-26 14:59:41:202] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:59:41:202] [DEBUG] [method:org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.getSaslPropertiesResolver(DataTransferSaslUtil.java:182)]DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[2020-08-26 14:59:41:208] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@77c799e8
[2020-08-26 14:59:41:231] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:59:41:231] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:59:41:231] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.71:8020
[2020-08-26 14:59:41:279] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (3257516) connection to /192.168.163.71:8020 from root: starting, having connections 1
[2020-08-26 14:59:41:281] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (3257516) connection to /192.168.163.71:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:59:41:285] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (3257516) connection to /192.168.163.71:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:59:41:291] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (3257516) connection to /192.168.163.71:8020 from root got value #0
[2020-08-26 14:59:41:291] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (3257516) connection to /192.168.163.71:8020 from root got value #1
[2020-08-26 14:59:41:296] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:59:41:296] [DEBUG] [method:org.apache.hadoop.io.retry.RetryInvocationHandler.log(RetryInvocationHandler.java:400)]Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.71:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1727)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1352)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:881)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2036)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2034)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1717)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1437)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1434)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:148)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1686)
	at com.ebchinatech.util.MergeFilePath.getPatternFile(MergeFilePath.java:48)
	at com.ebchinatech.util.MergeFilePath.merge(MergeFilePath.java:93)
	at com.ebchinatech.util.MergeUtil.lambda$merge$0(MergeUtil.java:109)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2020-08-26 14:59:41:297] [DEBUG] [method:org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:76)]multipleLinearRandomRetry = null
[2020-08-26 14:59:41:298] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63)]getting client out of cache: org.apache.hadoop.ipc.Client@77c799e8
[2020-08-26 14:59:41:298] [WARN ] [method:org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor.failover(RetryInvocationHandler.java:218)]A failover has occurred since the start of call #1 ClientNamenodeProtocolTranslatorPB.getFileInfo over /192.168.163.72:8020
[2020-08-26 14:59:41:298] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:59:41:298] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:484)]The ping interval is 60000 ms.
[2020-08-26 14:59:41:299] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:779)]Connecting to /192.168.163.72:8020
[2020-08-26 14:59:41:300] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1054)]IPC Client (3257516) connection to /192.168.163.72:8020 from root: starting, having connections 2
[2020-08-26 14:59:41:300] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (3257516) connection to /192.168.163.72:8020 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:59:41:301] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (3257516) connection to /192.168.163.72:8020 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
[2020-08-26 14:59:41:304] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (3257516) connection to /192.168.163.72:8020 from root got value #1
[2020-08-26 14:59:41:304] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (3257516) connection to /192.168.163.72:8020 from root got value #0
[2020-08-26 14:59:41:304] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 6ms
[2020-08-26 14:59:41:304] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getFileInfo took 6ms
[2020-08-26 14:59:41:330] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (3257516) connection to /192.168.163.72:8020 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:59:41:330] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1117)]IPC Client (3257516) connection to /192.168.163.72:8020 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing
[2020-08-26 14:59:41:331] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (3257516) connection to /192.168.163.72:8020 from root got value #3
[2020-08-26 14:59:41:332] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 14:59:41:332] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1171)]IPC Client (3257516) connection to /192.168.163.72:8020 from root got value #2
[2020-08-26 14:59:41:332] [DEBUG] [method:org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:248)]Call: getListing took 2ms
[2020-08-26 14:59:51:297] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (3257516) connection to /192.168.163.71:8020 from root: closed
[2020-08-26 14:59:51:297] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (3257516) connection to /192.168.163.71:8020 from root: stopped, remaining connections 1
[2020-08-26 14:59:51:336] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.close(Client.java:1253)]IPC Client (3257516) connection to /192.168.163.72:8020 from root: closed
[2020-08-26 14:59:51:336] [DEBUG] [method:org.apache.hadoop.ipc.Client$Connection.run(Client.java:1072)]IPC Client (3257516) connection to /192.168.163.72:8020 from root: stopped, remaining connections 0
[2020-08-26 15:04:24:403] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@77c799e8
[2020-08-26 15:04:24:418] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:97)]stopping client from cache: org.apache.hadoop.ipc.Client@77c799e8
[2020-08-26 15:04:24:419] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:103)]removing client from cache: org.apache.hadoop.ipc.Client@77c799e8
[2020-08-26 15:04:24:421] [DEBUG] [method:org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:110)]stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@77c799e8
[2020-08-26 15:04:24:424] [DEBUG] [method:org.apache.hadoop.ipc.Client.stop(Client.java:1306)]Stopping client
[2020-08-26 15:04:24:437] [DEBUG] [method:org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:84)]ShutdownHookManger complete shutdown.
